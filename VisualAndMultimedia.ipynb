{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "OJz2YgjK_Xr6",
        "NGHhh4RPqWZC",
        "aNWtcRsoqPOn",
        "3SSewQw5_zlq",
        "c4hvf_O89WIn",
        "O5Ol6MJF9bY3",
        "wMIQajsl9fnj",
        "QkoAzjES91ej",
        "xy5Vml1I9_f_",
        "w_7TsiV_-P5n",
        "X3Qkq-azfrjY",
        "KNJ52_hqh2Ov",
        "3zVRBPQVvjPb",
        "H8drxfOTvmAC"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DamianoGiani/EGVSR/blob/master/VisualAndMultimedia.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Import"
      ],
      "metadata": {
        "id": "OJz2YgjK_Xr6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "u4FllqYKRY7t"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "import cv2\n",
        "import time\n",
        "import os, sys\n",
        "from PIL import Image\n",
        "import pickle\n",
        "import torch.nn as nn\n",
        "import pickle\n",
        "import torch.nn.functional as F\n",
        "import functools\n",
        "import glob\n",
        "from skimage.metrics import structural_similarity as ssim\n",
        "import time\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "create folders to store results"
      ],
      "metadata": {
        "id": "ql4x9r-RLTDg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p /content/dataset/foreground/{GT,Gaussian}/foreground\n",
        "!mkdir -p /content/dataset/background/{GT,Gaussian}/background\n",
        "!mkdir -p /content/dataset/full/{GT,Gaussian}/full\n",
        "!mkdir video\n",
        "!mkdir -p results/{MyFirstMod,MySecondMod}/{EGVSR_iter420000,MyG_iter12000}"
      ],
      "metadata": {
        "id": "xxDYXJJ-jLFX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ca1a07f-d52d-42cb-9729-349b18f93682"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory â€˜videoâ€™: File exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#import shutil\n",
        "#%cd /content\n",
        "#shutil.rmtree('/content/results/MyThirdMod/EGVSR_iter420000')"
      ],
      "metadata": {
        "id": "GKMHLlLFLBdG"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "import the video that will be processed"
      ],
      "metadata": {
        "id": "9GDu1cCALqz0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -nc https://github.com/DamianoGiani/VisualAndMultimediaRecognition/blob/main/video/footballMatchTrim.mp4?raw=true -O video/footballMatch.mp4 #3840*2160 frame dimension\n",
        "#!wget -nc https://github.com/DamianoGiani/VisualAndMultimediaRecognition/blob/main/video/calcio_Trim.mp4?raw=true -O video/footballMatch.mp4 #1280*720 frame dimension"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G5Aywsb7UbP4",
        "outputId": "95f4075a-8cf7-4184-a1cb-a7ef02e439a7"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File â€˜video/footballMatch.mp4â€™ already there; not retrieving.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Separate Background and Foreground using yolov5"
      ],
      "metadata": {
        "id": "WRkeZLtMjZSv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "load YOLO model"
      ],
      "metadata": {
        "id": "01WpIjRaLkjs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "yolo = torch.hub.load('ultralytics/yolov5', 'yolov5s',)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yt7Ojz_1WmmF",
        "outputId": "02d0631b-e5f2-40e9-b90a-9608aa6d1afa"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/ultralytics_yolov5_master\n",
            "\u001b[31m\u001b[1mrequirements:\u001b[0m tqdm>=4.64.0 not found and is required by YOLOv5, attempting auto-update...\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tqdm>=4.64.0\n",
            "  Downloading tqdm-4.64.0-py2.py3-none-any.whl (78 kB)\n",
            "Installing collected packages: tqdm\n",
            "  Attempting uninstall: tqdm\n",
            "    Found existing installation: tqdm 4.56.0\n",
            "    Uninstalling tqdm-4.56.0:\n",
            "      Successfully uninstalled tqdm-4.56.0\n",
            "Successfully installed tqdm-4.64.0\n",
            "\n",
            "\u001b[31m\u001b[1mrequirements:\u001b[0m 1 package updated per /root/.cache/torch/hub/ultralytics_yolov5_master/requirements.txt\n",
            "\u001b[31m\u001b[1mrequirements:\u001b[0m âš ï¸ \u001b[1mRestart runtime or rerun command for updates to take effect\u001b[0m\n",
            "\n",
            "YOLOv5 ðŸš€ 2022-9-2 Python-3.7.13 torch-1.7.1 CUDA:0 (Tesla T4, 15110MiB)\n",
            "\n",
            "Fusing layers... \n",
            "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients\n",
            "Adding AutoShape... \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##TestSet Creation"
      ],
      "metadata": {
        "id": "UNcn2LgU_n4X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "start_time=time.time()\n",
        "vidcap = cv2.VideoCapture('/content/video/footballMatch.mp4')\n",
        "success,f = vidcap.read()\n",
        "h,w,_=f.shape\n",
        "frameSize =(w,h)\n",
        "count = 0\n",
        "cv2.waitKey(20)\n",
        "while success:\n",
        "    results = yolo(f)    \n",
        "    A=results.xyxyn[0].data.cpu().numpy() #coordinate of the bounding box \n",
        "    B=np.array(f) \n",
        "    for i in range(A.shape[0]):\n",
        "        if (A[i][5])==0.0 or (A[i][5]==32.0): #choose only bounding box with people and ball\n",
        "            f[int(A[i][1]*(f.shape[0])):int(A[i][3]*(f.shape[0])),int(A[i][0]*(f.shape[1])):int(A[i][2]*(f.shape[1]))]= (0, 0, 0)  \n",
        "    cv2.imwrite(\"/content/dataset/full/GT/full/frame%d.png\" % count, B)\n",
        "    B=B-f\n",
        "    cv2.imwrite(\"/content/dataset/background/GT/background/frame%d.png\" % count, f)\n",
        "    cv2.imwrite(\"/content/dataset/foreground/GT/foreground/frame%d.png\" % count, B)# save frame as png file      \n",
        "    success,f = vidcap.read()    \n",
        "    count += 1\n",
        "    cv2.waitKey(20)\n",
        "    if count > 14:\n",
        "      finish_time=time.time()-start_time\n",
        "      print('time required to process a frame is '+str(finish_time/15))\n",
        "      break"
      ],
      "metadata": {
        "id": "TQjAeHCvXCWl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47eefef5-5958-49ec-e456-c1f0648a9fff"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time required to process a frame is 0.9262630621592204\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###frame size reduction"
      ],
      "metadata": {
        "id": "NGHhh4RPqWZC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#reduces the images to 1/4 of their size\n",
        "def reduceFrameDimension(pathHR,pathLR):\n",
        "  list_of_files = filter( os.path.isfile,\n",
        "                        glob.glob(pathHR + '*') )\n",
        "# Sort list of files based on last modification time in ascending order\n",
        "  list_of_files = sorted( list_of_files,\n",
        "                        key = os.path.getmtime)\n",
        "  quality_val=40\n",
        "  for image in list_of_files:   \n",
        "    img = Image.open(image)\n",
        "    img.thumbnail((img.size[0]/4,img.size[1]/4))\n",
        "    name = os.path.split(image)  \n",
        "    file_path = os.path.join(pathLR, name[1])  \n",
        "    img.save(file_path, quality=quality_val)"
      ],
      "metadata": {
        "id": "1mUSJ_ccPQi9"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#reduces the background frames to 1/4 of their original size\n",
        "pathHRBackGround='/content/dataset/background/GT/background/'\n",
        "pathLRBackGround = '/content/dataset/background/Gaussian/background'\n",
        "reduceFrameDimension(pathHRBackGround,pathLRBackGround)\n",
        "\n",
        "#reduces the foreground frames to 1/4 of their original size\n",
        "pathHRForeground='/content/dataset/foreground/GT/foreground/'\n",
        "pathLRForeground = '/content/dataset/foreground/Gaussian/foreground'\n",
        "reduceFrameDimension(pathHRForeground,pathLRForeground)\n",
        "\n",
        "#reduces the full frames to 1/4 of their original size\n",
        "pathHRFullFrame='/content/dataset/full/GT/full/'\n",
        "pathLRFullFrame = '/content/dataset/full/Gaussian/full'\n",
        "reduceFrameDimension(pathHRFullFrame,pathLRFullFrame)"
      ],
      "metadata": {
        "id": "upMXRrfnPoYp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###create video with background,foreground and full frames"
      ],
      "metadata": {
        "id": "aNWtcRsoqPOn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "out = cv2.VideoWriter('/content/video/full.avi',cv2.VideoWriter_fourcc(*'DIVX'), 30, frameSize)\n",
        "\n",
        "for filename in sorted(glob.glob('/content/dataset/full/GT/full/*.png'), key=os.path.getmtime):\n",
        "    img = cv2.imread(filename)\n",
        "    out.write(img)\n",
        "\n",
        "out.release()\n",
        "\n",
        "\n",
        "\n",
        "out = cv2.VideoWriter('/content/video/background.avi',cv2.VideoWriter_fourcc(*'DIVX'), 30, frameSize)\n",
        "\n",
        "for filename in sorted(glob.glob('/content/dataset/background/GT/background/*.png'), key=os.path.getmtime):\n",
        "    img = cv2.imread(filename)\n",
        "    out.write(img)\n",
        "\n",
        "out.release()\n",
        "\n",
        "\n",
        "out = cv2.VideoWriter('/content/video/foreground.avi',cv2.VideoWriter_fourcc(*'DIVX'), 30, frameSize)\n",
        "\n",
        "for filename in sorted(glob.glob('/content/dataset/foreground/GT/foreground/*.png'), key=os.path.getmtime):\n",
        "    img = cv2.imread(filename)\n",
        "    out.write(img)\n",
        "\n",
        "out.release()"
      ],
      "metadata": {
        "id": "x4xxmsz2XLHh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#EGVSR"
      ],
      "metadata": {
        "id": "_j5EifmKsWMp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "import my Mod of EGVSR with all the utilities"
      ],
      "metadata": {
        "id": "UuWjHzUG_t9N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/DamianoGiani/EGVSR.git"
      ],
      "metadata": {
        "id": "MbAhlJr6sBhA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##install the requirements"
      ],
      "metadata": {
        "id": "3SSewQw5_zlq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install -r /content/EGVSR/requirements.txt"
      ],
      "metadata": {
        "id": "1EvEwx2JsX8F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install -r /content/EGVSR/req.txt"
      ],
      "metadata": {
        "id": "JOFIlpw7smiv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Test my Mod of EGVSR and save the resulting frames"
      ],
      "metadata": {
        "id": "mrfvR1_I_81v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/EGVSR"
      ],
      "metadata": {
        "id": "TPjYNfOKwOI6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!sh test.sh BD EGVSR 002"
      ],
      "metadata": {
        "id": "YqcLPutbtiKW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Create videos of the resulting frames"
      ],
      "metadata": {
        "id": "cHKNO_JQSDqR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "out = cv2.VideoWriter('/content/video/resultFirstMethod.avi',cv2.VideoWriter_fourcc(*'DIVX'), 20, frameSize)\n",
        "\n",
        "for filename in sorted(glob.glob('/content/results/MyFirstMod/EGVSR_iter420000/*.png'), key=os.path.getmtime):\n",
        "    img = cv2.imread(filename)\n",
        "    out.write(img)\n",
        "\n",
        "out.release()"
      ],
      "metadata": {
        "id": "JOGEif2vO10o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out = cv2.VideoWriter('/content/video/resultSecondMethod.avi',cv2.VideoWriter_fourcc(*'DIVX'), 20, frameSize)\n",
        "\n",
        "for filename in sorted(glob.glob('/content/results/MySecondMod/EGVSR_iter420000/*.png'), key=os.path.getmtime):\n",
        "    img = cv2.imread(filename)\n",
        "    out.write(img)\n",
        "\n",
        "out.release()"
      ],
      "metadata": {
        "id": "O5FO1sC-pwji"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Modules Visualization"
      ],
      "metadata": {
        "id": "c4hvf_O89WIn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Utilities"
      ],
      "metadata": {
        "id": "O5Ol6MJF9bY3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BicubicUpsample(nn.Module):\n",
        "\n",
        "    def     __init__(self, scale_factor, a=-0.75):\n",
        "        super(BicubicUpsample, self).__init__()\n",
        "\n",
        "        # calculate weights\n",
        "        cubic = torch.FloatTensor([\n",
        "            [0, a, -2 * a, a],\n",
        "            [1, 0, -(a + 3), a + 2],\n",
        "            [0, -a, (2 * a + 3), -(a + 2)],\n",
        "            [0, 0, a, -a]\n",
        "        ])  # accord to Eq.(6) in the reference paper\n",
        "\n",
        "        kernels = [\n",
        "            torch.matmul(cubic, torch.FloatTensor([1, s, s ** 2, s ** 3]))\n",
        "            for s in [1.0*d/scale_factor for d in range(scale_factor)]\n",
        "        ]  # s = x - floor(x)\n",
        "\n",
        "        # register parameters\n",
        "        self.scale_factor = scale_factor\n",
        "        self.register_buffer('kernels', torch.stack(kernels))\n",
        "\n",
        "    def forward(self, input):\n",
        "        n, c, h, w = input.size()\n",
        "        s = self.scale_factor\n",
        "\n",
        "        # pad input (left, right, top, bottom)\n",
        "        input = F.pad(input, (1, 2, 1, 2), mode='replicate')\n",
        "\n",
        "        # calculate output (height)\n",
        "        kernel_h = self.kernels.repeat(c, 1).view(-1, 1, s, 1)\n",
        "        output = F.conv2d(input, kernel_h, stride=1, padding=0, groups=c)\n",
        "        output = output.reshape(\n",
        "            n, c, s, -1, w + 3).permute(0, 1, 3, 2, 4).reshape(n, c, -1, w + 3)\n",
        "\n",
        "        # calculate output (width)\n",
        "        kernel_w = self.kernels.repeat(c, 1).view(-1, 1, 1, s)\n",
        "        output = F.conv2d(output, kernel_w, stride=1, padding=0, groups=c)\n",
        "        output = output.reshape(\n",
        "            n, c, s, h * s, -1).permute(0, 1, 3, 4, 2).reshape(n, c, h * s, -1)\n",
        "\n",
        "        return output"
      ],
      "metadata": {
        "id": "2jpv95qEw3wK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ResidualBlock(nn.Module):\n",
        "    \"\"\" Residual block without batch normalization\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, nf=64):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(nf, nf, 3, 1, 1, bias=True),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(nf, nf, 3, 1, 1, bias=True))\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv(x) + x\n",
        "\n",
        "        return"
      ],
      "metadata": {
        "id": "ELLRfc6WxA72"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BaseSequenceGenerator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(BaseSequenceGenerator, self).__init__()\n",
        "\n",
        "    def generate_dummy_input(self, lr_size):\n",
        "        \"\"\" use for compute per-step FLOPs and speed\n",
        "            return random tensors that can be taken as input of <forward>\n",
        "        \"\"\"\n",
        "        return None\n",
        "\n",
        "    def forward(self, *args, **kwargs):\n",
        "        \"\"\" forward pass for a singe frame\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    def forward_sequence(self, lr_data):\n",
        "        \"\"\" forward pass for a whole sequence (for training)\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    def infer_sequence(self, lr_data, device):\n",
        "        \"\"\" infer for a whole sequence (for inference)\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "\n",
        "class BaseSequenceDiscriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(BaseSequenceDiscriminator, self).__init__()\n",
        "\n",
        "    def forward(self, *args, **kwargs):\n",
        "        \"\"\" forward pass for a singe frame\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    def forward_sequence(self, data, args_dict):\n",
        "        \"\"\" forward pass for a whole sequence (for training)\n",
        "        \"\"\"\n",
        "        pass\n"
      ],
      "metadata": {
        "id": "gPPSE1cL7_8y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_upsampling_func(scale=4, degradation='BI'):\n",
        "    if degradation == 'BI':\n",
        "        upsample_func = functools.partial(\n",
        "            F.interpolate, scale_factor=scale, mode='bilinear',\n",
        "            align_corners=False)\n",
        "\n",
        "    elif degradation == 'BD':\n",
        "        upsample_func = BicubicUpsample(scale_factor=scale)\n",
        "\n",
        "    else:\n",
        "        raise ValueError('Unrecognized degradation: {}'.format(degradation))\n",
        "\n",
        "    return upsample_func"
      ],
      "metadata": {
        "id": "eKhVOr1a89bt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Modules definition"
      ],
      "metadata": {
        "id": "wMIQajsl9fnj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###FNet"
      ],
      "metadata": {
        "id": "QkoAzjES91ej"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FNet(nn.Module):\n",
        "    \"\"\" Optical flow estimation network\n",
        "    \"\"\"\n",
        "    \n",
        "        \n",
        "    def __init__(self, in_nc):\n",
        "        super().__init__()\n",
        "        super(FNet, self).__init__()\n",
        "\n",
        "        self.encoder1 = nn.Sequential(\n",
        "            nn.Conv2d(2*in_nc, 32, 3, 1, 1, bias=True),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(32, 32, 3, 1, 1, bias=True),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.MaxPool2d(2, 2))\n",
        "\n",
        "        self.encoder2 = nn.Sequential(\n",
        "            nn.Conv2d(32, 64, 3, 1, 1, bias=True),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(64, 64, 3, 1, 1, bias=True),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.MaxPool2d(2, 2))\n",
        "\n",
        "        self.encoder3 = nn.Sequential(\n",
        "            nn.Conv2d(64, 128, 3, 1, 1, bias=True),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(128, 128, 3, 1, 1, bias=True),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.MaxPool2d(2, 2))\n",
        "\n",
        "        self.decoder1 = nn.Sequential(\n",
        "            nn.Conv2d(128, 256, 3, 1, 1, bias=True),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(256, 256, 3, 1, 1, bias=True),\n",
        "            nn.LeakyReLU(0.2, inplace=True))\n",
        "\n",
        "        self.decoder2 = nn.Sequential(\n",
        "            nn.Conv2d(256, 128, 3, 1, 1, bias=True),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(128, 128, 3, 1, 1, bias=True),\n",
        "            nn.LeakyReLU(0.2, inplace=True))\n",
        "\n",
        "        self.decoder3 = nn.Sequential(\n",
        "            nn.Conv2d(128, 64, 3, 1, 1, bias=True),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(64, 64, 3, 1, 1, bias=True),\n",
        "            nn.LeakyReLU(0.2, inplace=True))\n",
        "\n",
        "        self.flow = nn.Sequential(\n",
        "            nn.Conv2d(64, 32, 3, 1, 1, bias=True),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(32, 2, 3, 1, 1, bias=True))\n",
        "\n",
        "    def forward(self, x1, x2):\n",
        "        \"\"\" Compute optical flow from x1 to x2\n",
        "        \"\"\"\n",
        "\n",
        "        out = self.encoder1(torch.cat([x1, x2], dim=1))\n",
        "        out = self.encoder2(out)\n",
        "        out = self.encoder3(out)\n",
        "        out = F.interpolate(\n",
        "            self.decoder1(out), scale_factor=2, mode='bilinear', align_corners=False)\n",
        "        out = F.interpolate(\n",
        "            self.decoder2(out), scale_factor=2, mode='bilinear', align_corners=False)\n",
        "        out = F.interpolate(\n",
        "            self.decoder3(out), scale_factor=2, mode='bilinear', align_corners=False)\n",
        "        out = torch.tanh(self.flow(out)) * 24  # 24 is the max velocity\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "7OEw59NJ7WOv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fnet=FNet(3)\n",
        "print(fnet)\n",
        "with open(\"fnet.txt\", \"a\") as f:\n",
        "  print(fnet, file=f)\n"
      ],
      "metadata": {
        "id": "-5FG6sjj6kdf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###SRNet"
      ],
      "metadata": {
        "id": "xy5Vml1I9_f_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SRNet(nn.Module):\n",
        "    \"\"\" Reconstruction & Upsampling network\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_nc=3, out_nc=3, nf=64, nb=16, upsample_func=None,\n",
        "                 scale=4):\n",
        "        super(SRNet, self).__init__()\n",
        "\n",
        "        # input conv.\n",
        "        self.conv_in = nn.Sequential(\n",
        "            nn.Conv2d((scale**2 + 1) * in_nc, nf, 3, 1, 1, bias=True),\n",
        "            nn.ReLU(inplace=True))\n",
        "\n",
        "        # residual blocks\n",
        "        self.resblocks = nn.Sequential(*[ResidualBlock(nf) for _ in range(nb)])\n",
        "\n",
        "        # upsampling\n",
        "        self.conv_up = nn.Sequential(\n",
        "            nn.ConvTranspose2d(nf, nf, 3, 2, 1, output_padding=1, bias=True),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.ConvTranspose2d(nf, nf, 3, 2, 1, output_padding=1, bias=True),\n",
        "            nn.ReLU(inplace=True))\n",
        "\n",
        "        self.conv_up_cheap = nn.Sequential(\n",
        "            nn.PixelShuffle(4),\n",
        "            nn.ReLU(inplace=True))\n",
        "\n",
        "        # output conv.\n",
        "        self.conv_out = nn.Conv2d(4, out_nc, 3, 1, 1, bias=True)\n",
        "\n",
        "        # upsampling function\n",
        "        self.upsample_func = upsample_func\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\" lr_curr: the current lr data in shape nchw\n",
        "            hr_prev_tran: the previous transformed hr_data in shape n(4*4*c)hw\n",
        "        \"\"\"\n",
        "\n",
        "        out = self.conv_in(x)\n",
        "        out = self.resblocks(out)\n",
        "        out = self.conv_up_cheap(out)\n",
        "        out = self.conv_out(out)\n",
        "        # out += self.upsample_func(lr_curr)\n",
        "\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "oqDa1e1QwN7C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "srnet = SRNet()\n",
        "print(srnet)\n",
        "with open(\"srnet.txt\", \"a\") as f:\n",
        "  print(srnet, file=f)\n",
        "  "
      ],
      "metadata": {
        "id": "v443Bhtaw6W4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###FRNet"
      ],
      "metadata": {
        "id": "w_7TsiV_-P5n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FRNet(BaseSequenceGenerator):\n",
        "    \"\"\" Frame-recurrent network proposed in https://arxiv.org/abs/1801.04590\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_nc=3, out_nc=3, nf=64, nb=16, degradation='BI',\n",
        "                 scale=4):\n",
        "        super(FRNet, self).__init__()\n",
        "\n",
        "        self.scale = scale\n",
        "\n",
        "        # get upsampling function according to the degradation mode\n",
        "        self.upsample_func = get_upsampling_func(self.scale, degradation)\n",
        "\n",
        "        # define fnet & srnet\n",
        "        self.fnet = FNet(in_nc)\n",
        "        self.srnet = SRNet(in_nc, out_nc, nf, nb, self.upsample_func)\n",
        "\n",
        "    def generate_dummy_input(self, lr_size):\n",
        "        c, lr_h, lr_w = lr_size\n",
        "        s = self.scale\n",
        "\n",
        "        lr_curr = torch.rand(1, c, lr_h, lr_w, dtype=torch.float32)\n",
        "        lr_prev = torch.rand(1, c, lr_h, lr_w, dtype=torch.float32)\n",
        "        hr_prev = torch.rand(1, c, s * lr_h, s * lr_w, dtype=torch.float32)\n",
        "\n",
        "        data_dict = {\n",
        "            'lr_curr': lr_curr,\n",
        "            'lr_prev': lr_prev,\n",
        "            'hr_prev': hr_prev\n",
        "        }\n",
        "\n",
        "        return data_dict\n",
        "\n",
        "    def forward(self, lr_curr, lr_prev, hr_prev):\n",
        "        \"\"\"\n",
        "            Parameters:\n",
        "                :param lr_curr: the current lr data in shape nchw\n",
        "                :param lr_prev: the previous lr data in shape nchw\n",
        "                :param hr_prev: the previous hr data in shape nc(4h)(4w)\n",
        "        \"\"\"\n",
        "\n",
        "        # estimate lr flow (lr_curr -> lr_prev)\n",
        "        lr_flow = self.fnet(lr_curr, lr_prev)\n",
        "\n",
        "        # pad if size is not a multiple of 8\n",
        "        pad_h = lr_curr.size(2) - lr_curr.size(2) // 8 * 8\n",
        "        pad_w = lr_curr.size(3) - lr_curr.size(3) // 8 * 8\n",
        "        lr_flow_pad = F.pad(lr_flow, (0, pad_w, 0, pad_h), 'reflect')\n",
        "\n",
        "        # upsample lr flow\n",
        "        hr_flow = self.scale * self.upsample_func(lr_flow_pad)\n",
        "\n",
        "        # warp hr_prev\n",
        "        hr_prev_warp = backward_warp(hr_prev, hr_flow)\n",
        "\n",
        "        # compute hr_curr\n",
        "        hr_curr = self.srnet(lr_curr, space_to_depth(hr_prev_warp, self.scale))\n",
        "        concres= torch.cat([lr_curr, space_to_depth(hr_prev_warp)],1)\n",
        "        return hr_curr,concres\n",
        "\n",
        "    def forward_sequence(self, lr_data):\n",
        "        \"\"\"\n",
        "            Parameters:\n",
        "                :param lr_data: lr data in shape ntchw\n",
        "        \"\"\"\n",
        "\n",
        "        n, t, c, lr_h, lr_w = lr_data.size()\n",
        "        hr_h, hr_w = lr_h * self.scale, lr_w * self.scale\n",
        "\n",
        "        # calculate optical flows\n",
        "        lr_prev = lr_data[:, :-1, ...].reshape(n * (t - 1), c, lr_h, lr_w)\n",
        "        lr_curr = lr_data[:, 1:, ...].reshape(n * (t - 1), c, lr_h, lr_w)\n",
        "        file_name = \"sample.pkl\"\n",
        "\n",
        "        open_file = open(file_name, \"wb\")\n",
        "        pickle.dump(lr_curr, open_file)\n",
        "        open_file.close()\n",
        "        lr_flow = self.fnet(lr_curr, lr_prev)  # n*(t-1),2,h,w\n",
        "\n",
        "        # upsample lr flows\n",
        "        hr_flow = self.scale * self.upsample_func(lr_flow)\n",
        "        hr_flow = hr_flow.view(n, (t - 1), 2, hr_h, hr_w)\n",
        "\n",
        "        # compute the first hr data\n",
        "        hr_data = []\n",
        "        hr_prev = self.srnet(\n",
        "            lr_data[:, 0, ...],\n",
        "            torch.zeros(n, (self.scale**2)*c, lr_h, lr_w, dtype=torch.float32,\n",
        "                        device=lr_data.device))\n",
        "        hr_data.append(hr_prev)\n",
        "\n",
        "        # compute the remaining hr data\n",
        "        for i in range(1, t):\n",
        "            # warp hr_prev\n",
        "            hr_prev_warp = backward_warp(hr_prev, hr_flow[:, i - 1, ...])\n",
        "\n",
        "            # compute hr_curr\n",
        "            hr_curr = self.srnet(\n",
        "                lr_data[:, i, ...],\n",
        "                space_to_depth(hr_prev_warp, self.scale))\n",
        "\n",
        "            # save and update\n",
        "            hr_data.append(hr_curr)\n",
        "            hr_prev = hr_curr\n",
        "\n",
        "        hr_data = torch.stack(hr_data, dim=1)  # n,t,c,hr_h,hr_w\n",
        "\n",
        "        # construct output dict\n",
        "        ret_dict = {\n",
        "            'hr_data': hr_data,  # n,t,c,hr_h,hr_w\n",
        "            'hr_flow': hr_flow,  # n,t,2,hr_h,hr_w\n",
        "            'lr_prev': lr_prev,  # n(t-1),c,lr_h,lr_w\n",
        "            'lr_curr': lr_curr,  # n(t-1),c,lr_h,lr_w\n",
        "            'lr_flow': lr_flow,  # n(t-1),2,lr_h,lr_w\n",
        "        }\n",
        "\n",
        "        return ret_dict\n",
        "\n",
        "    def infer_sequence(self, lr_data, device):\n",
        "        \"\"\"\n",
        "            Parameters:\n",
        "                :param lr_data: torch.FloatTensor in shape tchw\n",
        "                :param device: torch.device\n",
        "\n",
        "                :return hr_seq: uint8 np.ndarray in shape tchw\n",
        "        \"\"\"\n",
        "\n",
        "        # setup params\n",
        "        tot_frm, c, h, w = lr_data.size()\n",
        "        s = self.scale\n",
        "\n",
        "        # forward\n",
        "        hr_seq = []\n",
        "        lr_prev = torch.zeros(1, c, h, w, dtype=torch.float32).to(device)\n",
        "        hr_prev = torch.zeros(\n",
        "            1, c, s * h, s * w, dtype=torch.float32).to(device)\n",
        "        tensorlist = []\n",
        "        for i in range(tot_frm):\n",
        "            with torch.no_grad():\n",
        "                self.eval()\n",
        "\n",
        "                lr_curr = lr_data[i: i + 1, ...].to(device)\n",
        "                hr_curr,concres = self.forward(lr_curr, lr_prev, hr_prev) ##la domanda Ã¨ cosa fare con questo concres? che Ã¨ un tensore attualmente\n",
        "                lr_prev, hr_prev = lr_curr, hr_curr\n",
        "                tensorlist.append(concres) #######non so se devo prima fare queeze in numpy\n",
        "                hr_frm = hr_curr.squeeze(0).cpu().numpy()  # chw|rgb|uint8\n",
        "\n",
        "            hr_seq.append(float32_to_uint8(hr_frm))\n",
        "        file_name = \"sample.pkl\"\n",
        "\n",
        "        open_file = open(file_name, \"wb\")\n",
        "        pickle.dump(tensorlist, open_file)\n",
        "        open_file.close()\n",
        "\n",
        "        return np.stack(hr_seq).transpose(0, 2, 3, 1)  # thwc\n"
      ],
      "metadata": {
        "id": "Azs_ecE-70yH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "frnet=FRNet()\n",
        "print(frnet)\n",
        "with open(\"frnet.txt\", \"a\") as f:\n",
        "  print(frnet, file=f)\n"
      ],
      "metadata": {
        "id": "Btyybo358DJ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Result"
      ],
      "metadata": {
        "id": "X3Qkq-azfrjY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content"
      ],
      "metadata": {
        "id": "T0wH_ZPZhHVM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "clonation of the code for PerceptualSimilarity measurements"
      ],
      "metadata": {
        "id": "2bOZ3-ZsS7Ue"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/richzhang/PerceptualSimilarity\n"
      ],
      "metadata": {
        "id": "OIT9pG2X9gvj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Methods definition of the metrics"
      ],
      "metadata": {
        "id": "HZxVM8DjhRgE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd PerceptualSimilarity\n",
        "import lpips"
      ],
      "metadata": {
        "id": "iuuRF_ar9qlj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def PSNR(path1,path2,mod=''):\n",
        "  PSNR=[]\n",
        "  PSNR1=[]\n",
        "  if mod=='baseline':    \n",
        "    list_of_files = filter( os.path.isfile,\n",
        "                        glob.glob(path1 + '*'))\n",
        "    list_of_files = sorted( list_of_files,\n",
        "                        key = os.path.getmtime)\n",
        "    list_of_files2 = filter( os.path.isfile,\n",
        "                        glob.glob(path2 + '*'))\n",
        "    list_of_files2 = sorted(list_of_files2,key = os.path.getmtime)   \n",
        "    j=0\n",
        "    for image in sorted( list_of_files,key = os.path.getmtime):      \n",
        "      f = cv2.imread(image)\n",
        "      g = cv2.imread(list_of_files2[j], 1)\n",
        "      psnr1= cv2.PSNR(f, g)\n",
        "      PSNR1.append(psnr1)\n",
        "      results = yolo(f)    \n",
        "      A=results.xyxyn[0].data.cpu().numpy()\n",
        "      j+=1    \n",
        "      for i in range(A.shape[0]):\n",
        "          c=np.array(f[int(A[i][1]*(f.shape[0])):int(A[i][3]*(f.shape[0])),int(A[i][0]*(f.shape[1])):int(A[i][2]*(f.shape[1]))])\n",
        "          d=np.array(g[int(A[i][1]*(f.shape[0])):int(A[i][3]*(f.shape[0])),int(A[i][0]*(f.shape[1])):int(A[i][2]*(f.shape[1]))]) \n",
        "          psnr= cv2.PSNR(c, d)\n",
        "          PSNR.append(psnr)\n",
        "  else:\n",
        "    images1 = [file for file in os.listdir(path1) if file.endswith(('png'))]  \n",
        "    images2 = [file for file in os.listdir(path2) if file.endswith(('png'))]\n",
        "    \n",
        "    for i in range(len(images1)):       \n",
        "        f = cv2.imread(path1+'/'+images1[i])\n",
        "        g = cv2.imread(path2+'/'+images2[i], 1)\n",
        "        psnr1= cv2.PSNR(f, g)\n",
        "        PSNR1.append(psnr1)\n",
        "        results = yolo(f)    \n",
        "        A=results.xyxyn[0].data.cpu().numpy()    \n",
        "        for i in range(A.shape[0]):\n",
        "            c=np.array(f[int(A[i][1]*(f.shape[0])):int(A[i][3]*(f.shape[0])),int(A[i][0]*(f.shape[1])):int(A[i][2]*(f.shape[1]))])\n",
        "            d=np.array(g[int(A[i][1]*(f.shape[0])):int(A[i][3]*(f.shape[0])),int(A[i][0]*(f.shape[1])):int(A[i][2]*(f.shape[1]))]) \n",
        "            psnr= cv2.PSNR(c, d)\n",
        "            PSNR.append(psnr)     \n",
        "  psnrpatch=np.sum(PSNR)/len(PSNR)\n",
        "  print(\"PSNR calcolato su patch Ã¨: \" + str(psnrpatch))\n",
        "  psnrfull=np.sum(PSNR1)/len(PSNR1)  \n",
        "  print(\"PSNR calcolato intera immagine Ã¨: \" + str(psnrfull));\n",
        "  return psnr,psnrfull"
      ],
      "metadata": {
        "id": "BBear6Ib0Xcm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mse(imageA, imageB):\n",
        " # the 'Mean Squared Error' between the two images is the sum of the squared difference between the two images\n",
        " mse_error = np.sum((imageA.astype(\"float\") - imageB.astype(\"float\")) ** 2)\n",
        " mse_error /= float(imageA.shape[0] * imageA.shape[1])\n",
        "\t\n",
        " # return the MSE. The lower the error, the more \"similar\" the two images are.\n",
        " return mse_error\n",
        "\n",
        "def compare(imageA, imageB):\n",
        " # Calculate the MSE and SSIM\n",
        " m = mse(imageA, imageB)\n",
        " s = ssim(imageA, imageB)\n",
        " return s"
      ],
      "metadata": {
        "id": "8G97WlPY4mHD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def SSIM(path1,path2,mod=''):  \n",
        "  SSIM=[]\n",
        "  MSE=[]\n",
        "  SSIMPatch=[]\n",
        "  MSEPatch=[]\n",
        "  PSNR=[]\n",
        "  PSNR1=[]\n",
        "  if mod=='baseline':    \n",
        "    list_of_files = filter( os.path.isfile,\n",
        "                        glob.glob(path1 + '*'))\n",
        "    list_of_files = sorted( list_of_files,\n",
        "                        key = os.path.getmtime)\n",
        "    list_of_files2 = filter( os.path.isfile,\n",
        "                        glob.glob(path2 + '*'))\n",
        "    list_of_files2 = sorted(list_of_files2,key = os.path.getmtime)   \n",
        "    j=0\n",
        "    for image in sorted( list_of_files,key = os.path.getmtime): \n",
        "        f = cv2.imread(image)\n",
        "        g = cv2.imread(list_of_files2[j], 1)\n",
        "        gray1 = cv2.cvtColor(f, cv2.COLOR_BGR2GRAY)\n",
        "        gray2 = cv2.cvtColor(g, cv2.COLOR_BGR2GRAY)    \n",
        "        mse_value = mse(gray1, gray2)\n",
        "        ssim_value = compare(gray1, gray2)\n",
        "        SSIM.append(ssim_value)\n",
        "        MSE.append(mse_value)\n",
        "        results = yolo(f)    \n",
        "        A=results.xyxyn[0].data.cpu().numpy()\n",
        "        j+=1         \n",
        "        for i in range(A.shape[0]):\n",
        "          c=np.array(gray1[int(A[i][1]*(f.shape[0])):int(A[i][3]*(f.shape[0])),int(A[i][0]*(f.shape[1])):int(A[i][2]*(f.shape[1]))])\n",
        "          d=np.array(gray2[int(A[i][1]*(f.shape[0])):int(A[i][3]*(f.shape[0])),int(A[i][0]*(f.shape[1])):int(A[i][2]*(f.shape[1]))]) \n",
        "          mse_valuepatch = mse(c, d)\n",
        "          ssim_valuepatch = compare(c, d)\n",
        "          SSIMPatch.append(ssim_valuepatch)\n",
        "          MSEPatch.append(mse_valuepatch)\n",
        "  \n",
        "  else:\n",
        "    images1 = [file for file in os.listdir(path1) if file.endswith(('png'))]\n",
        "    images2 = [file for file in os.listdir(path2) if file.endswith(('png'))]\n",
        "    for i in range(len(images1)):\n",
        "        f = cv2.imread(path1+'/'+images1[i])\n",
        "        g = cv2.imread(path2+'/'+images2[i], 1)\n",
        "        gray1 = cv2.cvtColor(f, cv2.COLOR_BGR2GRAY)\n",
        "        gray2 = cv2.cvtColor(g, cv2.COLOR_BGR2GRAY)    \n",
        "        mse_value = mse(gray1, gray2)\n",
        "        ssim_value = compare(gray1, gray2)\n",
        "        SSIM.append(ssim_value)\n",
        "        MSE.append(mse_value)\n",
        "        results = yolo(f)    \n",
        "        A=results.xyxyn[0].data.cpu().numpy()         \n",
        "        for i in range(A.shape[0]):\n",
        "          c=np.array(gray1[int(A[i][1]*(f.shape[0])):int(A[i][3]*(f.shape[0])),int(A[i][0]*(f.shape[1])):int(A[i][2]*(f.shape[1]))])\n",
        "          d=np.array(gray2[int(A[i][1]*(f.shape[0])):int(A[i][3]*(f.shape[0])),int(A[i][0]*(f.shape[1])):int(A[i][2]*(f.shape[1]))]) \n",
        "          mse_valuepatch = mse(c, d)\n",
        "          ssim_valuepatch = compare(c, d)\n",
        "          SSIMPatch.append(ssim_valuepatch)\n",
        "          MSEPatch.append(mse_valuepatch)\n",
        "  \n",
        "  ssimtot=np.sum(SSIM)/len(SSIM)\n",
        "  print(\"SSIM calcolato su intere immagini Ã¨: \" + str(ssimtot))  \n",
        "   \n",
        "  msetot=np.sum(MSE)/len(MSE)\n",
        "  #print(\"MSE calcolato su intere immagini Ã¨: \" + str(msetot))\n",
        "  \n",
        "  ssimtotpatch=np.sum(SSIMPatch)/len(SSIMPatch)\n",
        "  print(\"SSIM calcolato su Patch Ã¨: \" + str(ssimtotpatch));         \n",
        "  msetotpatch=np.sum(MSEPatch)/len(MSEPatch)\n",
        "  #print(\"MSE calcolato su Patch Ã¨: \" + str(msetotpatch));   \n",
        "  return ssimtot,msetot,ssimtotpatch,msetotpatch"
      ],
      "metadata": {
        "id": "2IovtwzA3ovS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def DIST(path1,path2,mod=''):  \n",
        "  \n",
        "  DIST=[]\n",
        "  DISTPatch=[]\n",
        "  loss_fn = lpips.LPIPS(net='alex')\n",
        "  loss_fn.cuda()\n",
        "\n",
        "  if mod=='baseline':    \n",
        "    list_of_files = filter( os.path.isfile,\n",
        "                        glob.glob(path1 + '*'))\n",
        "    list_of_files = sorted( list_of_files,\n",
        "                        key = os.path.getmtime)\n",
        "    list_of_files2 = filter( os.path.isfile,\n",
        "                        glob.glob(path2 + '*'))\n",
        "    list_of_files2 = sorted(list_of_files2,key = os.path.getmtime)   \n",
        "    j=0\n",
        "    for image in sorted( list_of_files,key = os.path.getmtime):\n",
        "      f = cv2.imread(image)\n",
        "      g = cv2.imread(list_of_files2[j], 1)\n",
        "      results = yolo(f)    \n",
        "      A=results.xyxyn[0].data.cpu().numpy()\n",
        "      j+=1 \n",
        "      for i in range(A.shape[0]-1):\n",
        "          c=np.array(f[int(A[i][1]*(f.shape[0])):int(A[i][3]*(f.shape[0])),int(A[i][0]*(f.shape[1])):int(A[i][2]*(f.shape[1]))])\n",
        "          d=np.array(g[int(A[i][1]*(f.shape[0])):int(A[i][3]*(f.shape[0])),int(A[i][0]*(f.shape[1])):int(A[i][2]*(f.shape[1]))])\n",
        "          \n",
        "          h=(c / 255./2. - 1)[:, :, :, np.newaxis].transpose((3, 2, 0, 1))\n",
        "          hh=(d / 255./2. - 1)[:, :, :, np.newaxis].transpose((3, 2, 0, 1))\n",
        "          h=torch.Tensor(h)\n",
        "          hh=torch.Tensor(hh)\n",
        "          h = h.cuda()\n",
        "          hh = hh.cuda()\n",
        "          dist02 = loss_fn.forward(h,hh)\n",
        "          DISTPatch.append(dist02[0][0][0][0].cpu().detach().numpy())\n",
        "          torch.cuda.empty_cache()  \n",
        "     \n",
        "      k=(f / 255./2. - 1)[:, :, :, np.newaxis].transpose((3, 2, 0, 1))\n",
        "      kk=(g / 255./2. - 1)[:, :, :, np.newaxis].transpose((3, 2, 0, 1))\n",
        "      k=torch.Tensor(k)\n",
        "      kk=torch.Tensor(kk)\n",
        "      k = k.cuda()\n",
        "      kk = kk.cuda()\n",
        "      dist01 = loss_fn.forward(k,kk)\n",
        "      DIST.append(dist01[0][0][0][0].cpu().detach().numpy())\n",
        "      torch.cuda.empty_cache()\n",
        "  else:    \n",
        "\n",
        "    images1 = [file for file in os.listdir(path1) if file.endswith(('png'))]  \n",
        "    images2 = [file for file in os.listdir(path2) if file.endswith(('png'))]\n",
        "    for i in range(len(images1)):\n",
        "        f = cv2.imread(path1+'/'+images1[i])\n",
        "        g = cv2.imread(path2+'/'+images2[i], 1)\n",
        "        results = yolo(f)    \n",
        "        A=results.xyxyn[0].data.cpu().numpy()\n",
        "        for i in range(A.shape[0]-1):\n",
        "            c=np.array(f[int(A[i][1]*(f.shape[0])):int(A[i][3]*(f.shape[0])),int(A[i][0]*(f.shape[1])):int(A[i][2]*(f.shape[1]))])\n",
        "            d=np.array(g[int(A[i][1]*(f.shape[0])):int(A[i][3]*(f.shape[0])),int(A[i][0]*(f.shape[1])):int(A[i][2]*(f.shape[1]))])\n",
        "            \n",
        "            h=(c / 255./2. - 1)[:, :, :, np.newaxis].transpose((3, 2, 0, 1))\n",
        "            hh=(d / 255./2. - 1)[:, :, :, np.newaxis].transpose((3, 2, 0, 1))\n",
        "            h=torch.Tensor(h)\n",
        "            hh=torch.Tensor(hh)\n",
        "            h = h.cuda()\n",
        "            hh = hh.cuda()\n",
        "            dist02 = loss_fn.forward(h,hh)\n",
        "            DISTPatch.append(dist02[0][0][0][0].cpu().detach().numpy())\n",
        "            torch.cuda.empty_cache()          \n",
        "        k=(f / 255./2. - 1)[:, :, :, np.newaxis].transpose((3, 2, 0, 1))\n",
        "        kk=(g / 255./2. - 1)[:, :, :, np.newaxis].transpose((3, 2, 0, 1))\n",
        "        k=torch.Tensor(k)\n",
        "        kk=torch.Tensor(kk)\n",
        "        k = k.cuda()\n",
        "        kk = kk.cuda()\n",
        "        dist01 = loss_fn.forward(k,kk)\n",
        "        DIST.append(dist01[0][0][0][0].cpu().detach().numpy())\n",
        "        torch.cuda.empty_cache()\n",
        "        \n",
        "  dist1=np.sum(DIST)/len(DIST)  \n",
        "  print(\"Distance su tutta immagine : \" + str(dist1))\n",
        "  distpetch=np.sum(DISTPatch)/len(DISTPatch)  \n",
        "  print(\"Distance su patch : \" + str(distpetch))\n",
        "  return dist1 ,distpetch "
      ],
      "metadata": {
        "id": "7yAHfBjk84Nu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Call methods"
      ],
      "metadata": {
        "id": "KNJ52_hqh2Ov"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Results First Method"
      ],
      "metadata": {
        "id": "3zVRBPQVvjPb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PSNRBaselinepatch,baselineFull=PSNR('/content/dataset/full/GT/full/','/content/results/Full/EGVSR_iter420000/full/',mod='baseline')\n",
        "PSNRMyMod,MyModFull=PSNR('/content/dataset/full/GT/full','/content/results/MyFirstMod/EGVSR_iter420000')"
      ],
      "metadata": {
        "id": "9d2QS5fL0uD4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SSIMBaselineFull,MSEbaselineFull,SSIMBaselinepatch,MSEbaselinepatch=SSIM('/content/dataset/full/GT/full/','/content/results/Full/EGVSR_iter420000/full/',mod='baseline')\n",
        "SSIMMymodFull,MSEmymodFull,SSIMMyModpatch,MSEMuymodpatch=SSIM('/content/dataset/full/GT/full','/content/results/MyFirstMod/EGVSR_iter420000')"
      ],
      "metadata": {
        "id": "k6AACfNw5Drd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DISTBaselineFull,DISTBaselinePatch=DIST('/content/dataset/full/GT/full/','/content/results/Full/EGVSR_iter420000/full/',mod='baseline')\n",
        "DISTMymodFull,DISTMymodPatch=DIST('/content/dataset/full/GT/full','/content/results/MyFirstMod/EGVSR_iter420000')"
      ],
      "metadata": {
        "id": "bQpV-jAe-Puv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Results Second Method"
      ],
      "metadata": {
        "id": "H8drxfOTvmAC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PSNRBaselinepatch,baselineFull=PSNR('/content/dataset/full/GT/full/','/content/results/Full/EGVSR_iter420000/full/',mod='baseline')\n",
        "PSNRMyMod,MyModFull=PSNR('/content/dataset/full/GT/full','/content/results/MySecondMod/EGVSR_iter420000')"
      ],
      "metadata": {
        "id": "UlrqYR0Ovo6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SSIMBaselineFull,MSEbaselineFull,SSIMBaselinepatch,MSEbaselinepatch=SSIM('/content/dataset/full/GT/full/','/content/results/Full/EGVSR_iter420000/full/',mod='baseline')\n",
        "SSIMMymodFull,MSEmymodFull,SSIMMyModpatch,MSEMuymodpatch=SSIM('/content/dataset/full/GT/full','/content/results/MySecondMod/EGVSR_iter420000')"
      ],
      "metadata": {
        "id": "cQPIXxpFvpBP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DISTBaselineFull,DISTBaselinePatch=DIST('/content/dataset/full/GT/full/','/content/results/Full/EGVSR_iter420000/full/',mod='baseline')\n",
        "DISTMymodFull,DISTMymodPatch=DIST('/content/dataset/full/GT/full','/content/results/MySecondMod/EGVSR_iter420000')"
      ],
      "metadata": {
        "id": "kTgr8oT7vpIV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}