{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DamianoGiani/EGVSR/blob/master/VisualAndMultimedia.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OJz2YgjK_Xr6"
      },
      "source": [
        "#Import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "u4FllqYKRY7t"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "import cv2\n",
        "import time\n",
        "import os, sys\n",
        "from PIL import Image\n",
        "import pickle\n",
        "import torch.nn as nn\n",
        "import pickle\n",
        "import torch.nn.functional as F\n",
        "import functools\n",
        "import glob\n",
        "from skimage.metrics import structural_similarity as ssim\n",
        "import time\n",
        "best=False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ql4x9r-RLTDg"
      },
      "source": [
        "create folders to store results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "xxDYXJJ-jLFX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80ce18d9-4fa1-42a2-820f-3e5b72859833"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "mkdir: cannot create directory â€˜videoâ€™: File exists\n"
          ]
        }
      ],
      "source": [
        "!mkdir -p /content/dataset/foreground/{GT,Gaussian}/foreground\n",
        "!mkdir -p /content/dataset/background/{GT,Gaussian}/background\n",
        "!mkdir -p /content/dataset/full/{GT,Gaussian}/full\n",
        "!mkdir video\n",
        "!mkdir -p results/MySecondMod/{EGVSR_iter12000,MyG_iter12000}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GKMHLlLFLBdG"
      },
      "outputs": [],
      "source": [
        "#import shutil\n",
        "#%cd /content\n",
        "#shutil.rmtree('/content/results/MyThirdMod/EGVSR_iter420000')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9GDu1cCALqz0"
      },
      "source": [
        "import the video that will be processed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G5Aywsb7UbP4",
        "outputId": "c7c509dd-8129-4f4a-f7b8-684f086f3868"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-09-15 17:02:26--  https://github.com/DamianoGiani/VisualAndMultimediaRecognition/blob/main/video/footballMatchTrim.mp4?raw=true\n",
            "Resolving github.com (github.com)... 140.82.112.3\n",
            "Connecting to github.com (github.com)|140.82.112.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://github.com/DamianoGiani/VisualAndMultimediaRecognition/raw/main/video/footballMatchTrim.mp4 [following]\n",
            "--2022-09-15 17:02:27--  https://github.com/DamianoGiani/VisualAndMultimediaRecognition/raw/main/video/footballMatchTrim.mp4\n",
            "Reusing existing connection to github.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/DamianoGiani/VisualAndMultimediaRecognition/main/video/footballMatchTrim.mp4 [following]\n",
            "--2022-09-15 17:02:27--  https://raw.githubusercontent.com/DamianoGiani/VisualAndMultimediaRecognition/main/video/footballMatchTrim.mp4\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.108.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 25689187 (24M) [application/octet-stream]\n",
            "Saving to: â€˜video/footballMatch.mp4â€™\n",
            "\n",
            "video/footballMatch 100%[===================>]  24.50M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2022-09-15 17:02:27 (165 MB/s) - â€˜video/footballMatch.mp4â€™ saved [25689187/25689187]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget -nc https://github.com/DamianoGiani/VisualAndMultimediaRecognition/blob/main/video/footballMatchTrim.mp4?raw=true -O video/footballMatch.mp4 #3840*2160 frame dimension\n",
        "#!wget -nc https://github.com/DamianoGiani/VisualAndMultimediaRecognition/blob/main/video/calcio_Trim.mp4?raw=true -O video/footballMatch.mp4 #1280*720 frame dimension"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WRkeZLtMjZSv"
      },
      "source": [
        "#Separate Background and Foreground using yolov5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "01WpIjRaLkjs"
      },
      "source": [
        "load YOLO model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243,
          "referenced_widgets": [
            "2ddec4b305f843b9aaa7c7b128f4c040",
            "937d009a7e334e23be468aa50c726384",
            "a7242a94ab22411dac3a2dc34cb7d844",
            "0d2e2b190af44df3a4b274dd5e44268a",
            "10098d7fd8324b94aa87ec58326d3ce5",
            "8a64b53db0864cc293c35d45ccd0dcb3",
            "f082072d01e3475eb4cc06092ae2d33c",
            "a3da4a8583e44c44b579a48362a0c5d0",
            "1c308767d2204aba8c1c3f625c46496b",
            "7277c97947904319b6c7b68c44150735",
            "df6e01414f8d4d13a0a4154fff1f3d92"
          ]
        },
        "id": "yt7Ojz_1WmmF",
        "outputId": "175debc4-1f0d-429f-8c43-eeb3bee79de1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/hub.py:267: UserWarning: You are about to download and run code from an untrusted repository. In a future release, this won't be allowed. To add the repository to your trusted list, change the command to {calling_fn}(..., trust_repo=False) and a command prompt will appear asking for an explicit confirmation of trust, or load(..., trust_repo=True), which will assume that the prompt is to be answered with 'yes'. You can also use load(..., trust_repo='check') which will only prompt for confirmation if the repo is not already trusted. This will eventually be the default behaviour\n",
            "  \"You are about to download and run code from an untrusted repository. In a future release, this won't \"\n",
            "Downloading: \"https://github.com/ultralytics/yolov5/zipball/master\" to /root/.cache/torch/hub/master.zip\n",
            "YOLOv5 ðŸš€ 2022-9-15 Python-3.7.14 torch-1.12.1+cu113 CUDA:0 (Tesla T4, 15110MiB)\n",
            "\n",
            "Downloading https://github.com/ultralytics/yolov5/releases/download/v6.2/yolov5s.pt to yolov5s.pt...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0.00/14.1M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2ddec4b305f843b9aaa7c7b128f4c040"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Fusing layers... \n",
            "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients\n",
            "Adding AutoShape... \n"
          ]
        }
      ],
      "source": [
        "yolo = torch.hub.load('ultralytics/yolov5', 'yolov5s',)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UNcn2LgU_n4X"
      },
      "source": [
        "##TestSet Creation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TQjAeHCvXCWl",
        "outputId": "329857d5-7e6d-4dfe-b1b6-3e32f06aa31f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time required to process a frame is 0.8953097979227702\n"
          ]
        }
      ],
      "source": [
        "start_time=time.time()\n",
        "vidcap = cv2.VideoCapture('/content/video/footballMatch.mp4')\n",
        "success,f = vidcap.read()\n",
        "h,w,_=f.shape\n",
        "frameSize =(w,h)\n",
        "count = 0\n",
        "cv2.waitKey(20)\n",
        "while success:\n",
        "    results = yolo(f)    \n",
        "    A=results.xyxyn[0].data.cpu().numpy() #coordinate of the bounding box \n",
        "    B=np.array(f) \n",
        "    for i in range(A.shape[0]):\n",
        "        if (A[i][5])==0.0 or (A[i][5]==32.0): #choose only bounding box with people and ball\n",
        "            f[int(A[i][1]*(f.shape[0])):int(A[i][3]*(f.shape[0])),int(A[i][0]*(f.shape[1])):int(A[i][2]*(f.shape[1]))]= (0, 0, 0)  \n",
        "    cv2.imwrite(\"/content/dataset/full/GT/full/frame%d.png\" % count, B)\n",
        "    B=B-f\n",
        "    cv2.imwrite(\"/content/dataset/background/GT/background/frame%d.png\" % count, f)\n",
        "    cv2.imwrite(\"/content/dataset/foreground/GT/foreground/frame%d.png\" % count, B)# save frame as png file      \n",
        "    success,f = vidcap.read()    \n",
        "    count += 1\n",
        "    cv2.waitKey(20)\n",
        "    if count > 14:\n",
        "      finish_time=time.time()-start_time\n",
        "      print('time required to process a frame is '+str(finish_time/15))\n",
        "      break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NGHhh4RPqWZC"
      },
      "source": [
        "###frame size reduction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "1mUSJ_ccPQi9"
      },
      "outputs": [],
      "source": [
        "#reduces the images to 1/4 of their size\n",
        "def reduceFrameDimension(pathHR,pathLR):\n",
        "  list_of_files = filter( os.path.isfile,\n",
        "                        glob.glob(pathHR + '*') )\n",
        "# Sort list of files based on last modification time in ascending order\n",
        "  list_of_files = sorted( list_of_files,\n",
        "                        key = os.path.getmtime)\n",
        "  quality_val=40\n",
        "  for image in list_of_files:   \n",
        "    img = Image.open(image)\n",
        "    img.thumbnail((img.size[0]/4,img.size[1]/4))\n",
        "    name = os.path.split(image)  \n",
        "    file_path = os.path.join(pathLR, name[1])  \n",
        "    img.save(file_path, quality=quality_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "upMXRrfnPoYp"
      },
      "outputs": [],
      "source": [
        "#reduces the background frames to 1/4 of their original size\n",
        "pathHRBackGround='/content/dataset/background/GT/background/'\n",
        "pathLRBackGround = '/content/dataset/background/Gaussian/background'\n",
        "reduceFrameDimension(pathHRBackGround,pathLRBackGround)\n",
        "\n",
        "#reduces the foreground frames to 1/4 of their original size\n",
        "pathHRForeground='/content/dataset/foreground/GT/foreground/'\n",
        "pathLRForeground = '/content/dataset/foreground/Gaussian/foreground'\n",
        "reduceFrameDimension(pathHRForeground,pathLRForeground)\n",
        "\n",
        "#reduces the full frames to 1/4 of their original size\n",
        "pathHRFullFrame='/content/dataset/full/GT/full/'\n",
        "pathLRFullFrame = '/content/dataset/full/Gaussian/full'\n",
        "reduceFrameDimension(pathHRFullFrame,pathLRFullFrame)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aNWtcRsoqPOn"
      },
      "source": [
        "###create video with background,foreground and full frames"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "x4xxmsz2XLHh"
      },
      "outputs": [],
      "source": [
        "out = cv2.VideoWriter('/content/video/full.avi',cv2.VideoWriter_fourcc(*'DIVX'), 30, frameSize)\n",
        "\n",
        "for filename in sorted(glob.glob('/content/dataset/full/GT/full/*.png'), key=os.path.getmtime):\n",
        "    img = cv2.imread(filename)\n",
        "    out.write(img)\n",
        "\n",
        "out.release()\n",
        "\n",
        "\n",
        "\n",
        "out = cv2.VideoWriter('/content/video/background.avi',cv2.VideoWriter_fourcc(*'DIVX'), 30, frameSize)\n",
        "\n",
        "for filename in sorted(glob.glob('/content/dataset/background/GT/background/*.png'), key=os.path.getmtime):\n",
        "    img = cv2.imread(filename)\n",
        "    out.write(img)\n",
        "\n",
        "out.release()\n",
        "\n",
        "\n",
        "out = cv2.VideoWriter('/content/video/foreground.avi',cv2.VideoWriter_fourcc(*'DIVX'), 30, frameSize)\n",
        "\n",
        "for filename in sorted(glob.glob('/content/dataset/foreground/GT/foreground/*.png'), key=os.path.getmtime):\n",
        "    img = cv2.imread(filename)\n",
        "    out.write(img)\n",
        "\n",
        "out.release()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_j5EifmKsWMp"
      },
      "source": [
        "#EGVSR"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UuWjHzUG_t9N"
      },
      "source": [
        "import my Mod of EGVSR with all the utilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MbAhlJr6sBhA",
        "outputId": "26d75e31-1a35-4019-a167-739ef8ecd615"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'EGVSR'...\n",
            "remote: Enumerating objects: 674, done.\u001b[K\n",
            "remote: Counting objects: 100% (423/423), done.\u001b[K\n",
            "remote: Compressing objects: 100% (232/232), done.\u001b[K\n",
            "remote: Total 674 (delta 255), reused 312 (delta 189), pack-reused 251\u001b[K\n",
            "Receiving objects: 100% (674/674), 177.80 MiB | 42.83 MiB/s, done.\n",
            "Resolving deltas: 100% (305/305), done.\n",
            "Checking out files: 100% (192/192), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/DamianoGiani/EGVSR.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3SSewQw5_zlq"
      },
      "source": [
        "##install the requirements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "JOFIlpw7smiv",
        "outputId": "5cade478-fd3d-4da1-ebf9-703a7a312982"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting absl-py==0.11.0\n",
            "  Downloading absl_py-0.11.0-py3-none-any.whl (127 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 127 kB 5.1 MB/s \n",
            "\u001b[?25hCollecting certifi==2020.12.5\n",
            "  Downloading certifi-2020.12.5-py2.py3-none-any.whl (147 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 147 kB 41.1 MB/s \n",
            "\u001b[?25hCollecting chardet==4.0.0\n",
            "  Downloading chardet-4.0.0-py2.py3-none-any.whl (178 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 178 kB 59.2 MB/s \n",
            "\u001b[?25hCollecting cycler==0.10.0\n",
            "  Downloading cycler-0.10.0-py2.py3-none-any.whl (6.5 kB)\n",
            "Requirement already satisfied: decorator==4.4.2 in /usr/local/lib/python3.7/dist-packages (from -r /content/EGVSR/req.txt (line 5)) (4.4.2)\n",
            "Collecting flow-vis==0.1\n",
            "  Downloading flow_vis-0.1-py3-none-any.whl (4.4 kB)\n",
            "Collecting future==0.18.2\n",
            "  Downloading future-0.18.2.tar.gz (829 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 829 kB 62.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: idna==2.10 in /usr/local/lib/python3.7/dist-packages (from -r /content/EGVSR/req.txt (line 8)) (2.10)\n",
            "Requirement already satisfied: imageio==2.9.0 in /usr/local/lib/python3.7/dist-packages (from -r /content/EGVSR/req.txt (line 9)) (2.9.0)\n",
            "Collecting jsonpatch==1.32\n",
            "  Downloading jsonpatch-1.32-py2.py3-none-any.whl (12 kB)\n",
            "Collecting jsonpointer==2.1\n",
            "  Downloading jsonpointer-2.1-py2.py3-none-any.whl (7.4 kB)\n",
            "Requirement already satisfied: kiwisolver in /usr/local/lib/python3.7/dist-packages (from -r /content/EGVSR/req.txt (line 12)) (1.4.4)\n",
            "Collecting lmdb==1.0.0\n",
            "  Downloading lmdb-1.0.0-cp37-cp37m-manylinux1_x86_64.whl (280 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 280 kB 67.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from -r /content/EGVSR/req.txt (line 14)) (3.2.2)\n",
            "Collecting mkl-fft==1.3.0\n",
            "  Downloading mkl_fft-1.3.0-1-cp37-cp37m-manylinux2014_x86_64.whl (240 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 240 kB 74.2 MB/s \n",
            "\u001b[?25hCollecting mkl-random==1.2.2\n",
            "  Downloading mkl_random-1.2.2-16-cp37-cp37m-manylinux2014_x86_64.whl (379 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 379 kB 69.9 MB/s \n",
            "\u001b[?25hCollecting mkl-service==2.3.0\n",
            "  Downloading mkl_service-2.3.0-10-cp37-cp37m-manylinux2014_x86_64.whl (61 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 61 kB 11.0 MB/s \n",
            "\u001b[?25hCollecting networkx==2.5\n",
            "  Downloading networkx-2.5-py3-none-any.whl (1.6 MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.6 MB 56.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from -r /content/EGVSR/req.txt (line 19)) (1.21.6)\n",
            "Collecting olefile==0.46\n",
            "  Downloading olefile-0.46.zip (112 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 112 kB 74.1 MB/s \n",
            "\u001b[?25hCollecting opencv-python==4.5.1.48\n",
            "  Downloading opencv_python-4.5.1.48-cp37-cp37m-manylinux2014_x86_64.whl (50.4 MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50.4 MB 1.9 MB/s \n",
            "\u001b[?25hCollecting pandas==1.2.1\n",
            "  Downloading pandas-1.2.1-cp37-cp37m-manylinux1_x86_64.whl (9.9 MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9.9 MB 50.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from -r /content/EGVSR/req.txt (line 23)) (7.1.2)\n",
            "Collecting pyparsing==2.4.7\n",
            "  Downloading pyparsing-2.4.7-py2.py3-none-any.whl (67 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 67 kB 7.2 MB/s \n",
            "\u001b[?25hCollecting PyQt5-sip==12.8.1\n",
            "  Downloading PyQt5_sip-12.8.1-cp37-cp37m-manylinux1_x86_64.whl (283 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 283 kB 67.9 MB/s \n",
            "\u001b[?25hCollecting python-dateutil==2.8.1\n",
            "  Downloading python_dateutil-2.8.1-py2.py3-none-any.whl (227 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 227 kB 78.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from -r /content/EGVSR/req.txt (line 27)) (2022.2.1)\n",
            "Collecting PyWavelets==1.1.1\n",
            "  Downloading PyWavelets-1.1.1-cp37-cp37m-manylinux1_x86_64.whl (4.4 MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4.4 MB 56.2 MB/s \n",
            "\u001b[?25hCollecting PyYAML==5.3.1\n",
            "  Downloading PyYAML-5.3.1.tar.gz (269 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 269 kB 73.2 MB/s \n",
            "\u001b[?25hCollecting pyzmq==22.0.3\n",
            "  Downloading pyzmq-22.0.3-cp37-cp37m-manylinux1_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.1 MB 63.3 MB/s \n",
            "\u001b[?25hCollecting requests==2.25.1\n",
            "  Downloading requests-2.25.1-py2.py3-none-any.whl (61 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 61 kB 9.8 MB/s \n",
            "\u001b[?25hCollecting scikit-image==0.18.1\n",
            "  Downloading scikit_image-0.18.1-cp37-cp37m-manylinux1_x86_64.whl (29.2 MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29.2 MB 100.2 MB/s \n",
            "\u001b[?25hCollecting scipy==1.6.0\n",
            "  Downloading scipy-1.6.0-cp37-cp37m-manylinux1_x86_64.whl (27.4 MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27.4 MB 133.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from -r /content/EGVSR/req.txt (line 34)) (1.15.0)\n",
            "Collecting tifffile==2021.1.8\n",
            "  Downloading tifffile-2021.1.8-py3-none-any.whl (158 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 158 kB 75.5 MB/s \n",
            "\u001b[?25hCollecting torch==1.7.1\n",
            "  Downloading torch-1.7.1-cp37-cp37m-manylinux1_x86_64.whl (776.8 MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 776.8 MB 16 kB/s \n",
            "\u001b[?25hCollecting torchfile==0.1.0\n",
            "  Downloading torchfile-0.1.0.tar.gz (5.2 kB)\n",
            "Collecting torchnet==0.0.4\n",
            "  Downloading torchnet-0.0.4.tar.gz (23 kB)\n",
            "Requirement already satisfied: torchsummary==1.5.1 in /usr/local/lib/python3.7/dist-packages (from -r /content/EGVSR/req.txt (line 39)) (1.5.1)\n",
            "Collecting torchvision==0.8.2\n",
            "  Downloading torchvision-0.8.2-cp37-cp37m-manylinux1_x86_64.whl (12.8 MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12.8 MB 58.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tornado in /usr/local/lib/python3.7/dist-packages (from -r /content/EGVSR/req.txt (line 41)) (5.1.1)\n",
            "Collecting tqdm==4.56.0\n",
            "  Downloading tqdm-4.56.0-py2.py3-none-any.whl (72 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 72 kB 1.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from -r /content/EGVSR/req.txt (line 43)) (4.1.1)\n",
            "Collecting urllib3==1.26.4\n",
            "  Downloading urllib3-1.26.4-py2.py3-none-any.whl (153 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 153 kB 78.1 MB/s \n",
            "\u001b[?25hCollecting visdom==0.1.8.9\n",
            "  Downloading visdom-0.1.8.9.tar.gz (676 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 676 kB 66.8 MB/s \n",
            "\u001b[?25hCollecting websocket-client==0.58.0\n",
            "  Downloading websocket_client-0.58.0-py2.py3-none-any.whl (61 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 61 kB 9.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: mkl in /usr/local/lib/python3.7/dist-packages (from mkl-fft==1.3.0->-r /content/EGVSR/req.txt (line 15)) (2019.0)\n",
            "Collecting dpcpp_cpp_rt\n",
            "  Downloading dpcpp_cpp_rt-2022.1.0-py2.py3-none-manylinux1_x86_64.whl (4.0 MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4.0 MB 51.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: intel-openmp==2022.1.0 in /usr/local/lib/python3.7/dist-packages (from dpcpp_cpp_rt->mkl-fft==1.3.0->-r /content/EGVSR/req.txt (line 15)) (2022.1.0)\n",
            "Collecting intel-cmplr-lic-rt==2022.1.0\n",
            "  Downloading intel_cmplr_lic_rt-2022.1.0-py2.py3-none-manylinux1_x86_64.whl (18 kB)\n",
            "Collecting intel-opencl-rt==2022.1.0\n",
            "  Downloading intel_opencl_rt-2022.1.0-py2.py3-none-manylinux1_x86_64.whl (232.4 MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 232.4 MB 7.2 kB/s \n",
            "\u001b[?25hCollecting intel-cmplr-lib-rt==2022.1.0\n",
            "  Downloading intel_cmplr_lib_rt-2022.1.0-py2.py3-none-manylinux1_x86_64.whl (37.2 MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 37.2 MB 246 kB/s \n",
            "\u001b[?25hCollecting tbb==2021.*\n",
            "  Downloading tbb-2021.6.0-py2.py3-none-manylinux1_x86_64.whl (4.0 MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4.0 MB 44.2 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: future, olefile, PyYAML, torchfile, torchnet, visdom\n",
            "  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for future: filename=future-0.18.2-py3-none-any.whl size=491070 sha256=c69d5ff8a42641c6fb160e4b9d7d1bd38f0fa5f8a98e1bcaf155f02281c6347b\n",
            "  Stored in directory: /root/.cache/pip/wheels/56/b0/fe/4410d17b32f1f0c3cf54cdfb2bc04d7b4b8f4ae377e2229ba0\n",
            "  Building wheel for olefile (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for olefile: filename=olefile-0.46-py2.py3-none-any.whl size=35432 sha256=e2329a3ff196b574422e02ca67aa0480262121008a5aebc79f9a69509bd69d73\n",
            "  Stored in directory: /root/.cache/pip/wheels/84/53/e6/37d90ccb3ad1a3ca98d2b17107e9fda401a7c541ea1eb6a65a\n",
            "  Building wheel for PyYAML (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for PyYAML: filename=PyYAML-5.3.1-cp37-cp37m-linux_x86_64.whl size=44636 sha256=9cadde00e8c9123d9094895d14ed38dd5729b62b7e56b8d9f90c84c0263459bc\n",
            "  Stored in directory: /root/.cache/pip/wheels/5e/03/1e/e1e954795d6f35dfc7b637fe2277bff021303bd9570ecea653\n",
            "  Building wheel for torchfile (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torchfile: filename=torchfile-0.1.0-py3-none-any.whl size=5709 sha256=3bed89b09032df783d63850d0591439a393b55c3b2825fe7cfed197d9cb2d1f5\n",
            "  Stored in directory: /root/.cache/pip/wheels/ac/5c/3a/a80e1c65880945c71fd833408cd1e9a8cb7e2f8f37620bb75b\n",
            "  Building wheel for torchnet (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torchnet: filename=torchnet-0.0.4-py3-none-any.whl size=29742 sha256=b7d4ac72c9876e4e5c59076dba325810dd9707efdb4ac6375439c272a2378efb\n",
            "  Stored in directory: /root/.cache/pip/wheels/93/17/b3/86db1d93e9dae198813aa79831b403e4844d67986cf93894b5\n",
            "  Building wheel for visdom (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for visdom: filename=visdom-0.1.8.9-py3-none-any.whl size=655250 sha256=490e14a8e16967ba3868664975ba67ffc65cb9d9d0e6a36bf82b9e0d6d70001b\n",
            "  Stored in directory: /root/.cache/pip/wheels/2d/d1/9b/cde923274eac9cbb6ff0d8c7c72fe30a3da9095a38fd50bbf1\n",
            "Successfully built future olefile PyYAML torchfile torchnet visdom\n",
            "Installing collected packages: urllib3, tbb, jsonpointer, intel-cmplr-lic-rt, chardet, certifi, websocket-client, torchfile, scipy, requests, pyzmq, python-dateutil, pyparsing, jsonpatch, intel-opencl-rt, intel-cmplr-lib-rt, cycler, visdom, torch, tifffile, PyWavelets, networkx, dpcpp-cpp-rt, tqdm, torchvision, torchnet, scikit-image, PyYAML, PyQt5-sip, pandas, opencv-python, olefile, mkl-service, mkl-random, mkl-fft, lmdb, future, flow-vis, absl-py\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "  Attempting uninstall: chardet\n",
            "    Found existing installation: chardet 3.0.4\n",
            "    Uninstalling chardet-3.0.4:\n",
            "      Successfully uninstalled chardet-3.0.4\n",
            "  Attempting uninstall: certifi\n",
            "    Found existing installation: certifi 2022.6.15\n",
            "    Uninstalling certifi-2022.6.15:\n",
            "      Successfully uninstalled certifi-2022.6.15\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.7.3\n",
            "    Uninstalling scipy-1.7.3:\n",
            "      Successfully uninstalled scipy-1.7.3\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.23.0\n",
            "    Uninstalling requests-2.23.0:\n",
            "      Successfully uninstalled requests-2.23.0\n",
            "  Attempting uninstall: pyzmq\n",
            "    Found existing installation: pyzmq 23.2.1\n",
            "    Uninstalling pyzmq-23.2.1:\n",
            "      Successfully uninstalled pyzmq-23.2.1\n",
            "  Attempting uninstall: python-dateutil\n",
            "    Found existing installation: python-dateutil 2.8.2\n",
            "    Uninstalling python-dateutil-2.8.2:\n",
            "      Successfully uninstalled python-dateutil-2.8.2\n",
            "  Attempting uninstall: pyparsing\n",
            "    Found existing installation: pyparsing 3.0.9\n",
            "    Uninstalling pyparsing-3.0.9:\n",
            "      Successfully uninstalled pyparsing-3.0.9\n",
            "  Attempting uninstall: cycler\n",
            "    Found existing installation: cycler 0.11.0\n",
            "    Uninstalling cycler-0.11.0:\n",
            "      Successfully uninstalled cycler-0.11.0\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.12.1+cu113\n",
            "    Uninstalling torch-1.12.1+cu113:\n",
            "      Successfully uninstalled torch-1.12.1+cu113\n",
            "  Attempting uninstall: tifffile\n",
            "    Found existing installation: tifffile 2021.11.2\n",
            "    Uninstalling tifffile-2021.11.2:\n",
            "      Successfully uninstalled tifffile-2021.11.2\n",
            "  Attempting uninstall: PyWavelets\n",
            "    Found existing installation: PyWavelets 1.3.0\n",
            "    Uninstalling PyWavelets-1.3.0:\n",
            "      Successfully uninstalled PyWavelets-1.3.0\n",
            "  Attempting uninstall: networkx\n",
            "    Found existing installation: networkx 2.6.3\n",
            "    Uninstalling networkx-2.6.3:\n",
            "      Successfully uninstalled networkx-2.6.3\n",
            "  Attempting uninstall: tqdm\n",
            "    Found existing installation: tqdm 4.64.1\n",
            "    Uninstalling tqdm-4.64.1:\n",
            "      Successfully uninstalled tqdm-4.64.1\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.13.1+cu113\n",
            "    Uninstalling torchvision-0.13.1+cu113:\n",
            "      Successfully uninstalled torchvision-0.13.1+cu113\n",
            "  Attempting uninstall: scikit-image\n",
            "    Found existing installation: scikit-image 0.18.3\n",
            "    Uninstalling scikit-image-0.18.3:\n",
            "      Successfully uninstalled scikit-image-0.18.3\n",
            "  Attempting uninstall: PyYAML\n",
            "    Found existing installation: PyYAML 6.0\n",
            "    Uninstalling PyYAML-6.0:\n",
            "      Successfully uninstalled PyYAML-6.0\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 1.3.5\n",
            "    Uninstalling pandas-1.3.5:\n",
            "      Successfully uninstalled pandas-1.3.5\n",
            "  Attempting uninstall: opencv-python\n",
            "    Found existing installation: opencv-python 4.6.0.66\n",
            "    Uninstalling opencv-python-4.6.0.66:\n",
            "      Successfully uninstalled opencv-python-4.6.0.66\n",
            "  Attempting uninstall: lmdb\n",
            "    Found existing installation: lmdb 0.99\n",
            "    Uninstalling lmdb-0.99:\n",
            "      Successfully uninstalled lmdb-0.99\n",
            "  Attempting uninstall: future\n",
            "    Found existing installation: future 0.16.0\n",
            "    Uninstalling future-0.16.0:\n",
            "      Successfully uninstalled future-0.16.0\n",
            "  Attempting uninstall: absl-py\n",
            "    Found existing installation: absl-py 1.2.0\n",
            "    Uninstalling absl-py-1.2.0:\n",
            "      Successfully uninstalled absl-py-1.2.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchtext 0.13.1 requires torch==1.12.1, but you have torch 1.7.1 which is incompatible.\n",
            "torchaudio 0.12.1+cu113 requires torch==1.12.1, but you have torch 1.7.1 which is incompatible.\u001b[0m\n",
            "Successfully installed PyQt5-sip-12.8.1 PyWavelets-1.1.1 PyYAML-5.3.1 absl-py-0.11.0 certifi-2020.12.5 chardet-4.0.0 cycler-0.10.0 dpcpp-cpp-rt-2022.1.0 flow-vis-0.1 future-0.18.2 intel-cmplr-lib-rt-2022.1.0 intel-cmplr-lic-rt-2022.1.0 intel-opencl-rt-2022.1.0 jsonpatch-1.32 jsonpointer-2.1 lmdb-1.0.0 mkl-fft-1.3.0 mkl-random-1.2.2 mkl-service-2.3.0 networkx-2.5 olefile-0.46 opencv-python-4.5.1.48 pandas-1.2.1 pyparsing-2.4.7 python-dateutil-2.8.1 pyzmq-22.0.3 requests-2.25.1 scikit-image-0.18.1 scipy-1.6.0 tbb-2021.6.0 tifffile-2021.1.8 torch-1.7.1 torchfile-0.1.0 torchnet-0.0.4 torchvision-0.8.2 tqdm-4.56.0 urllib3-1.26.4 visdom-0.1.8.9 websocket-client-0.58.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "certifi",
                  "chardet",
                  "cv2",
                  "cycler",
                  "dateutil",
                  "pandas",
                  "pyparsing",
                  "requests",
                  "scipy",
                  "skimage",
                  "torch",
                  "torchvision",
                  "tqdm",
                  "urllib3",
                  "yaml",
                  "zmq"
                ]
              }
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip3 install -r /content/EGVSR/req.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mrfvR1_I_81v"
      },
      "source": [
        "##Test my Mod of EGVSR and save the resulting frames"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TPjYNfOKwOI6",
        "outputId": "8b046c88-0500-4cfd-e337-3d1296ac756d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/EGVSR\n"
          ]
        }
      ],
      "source": [
        "%cd /content/EGVSR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YqcLPutbtiKW",
        "outputId": "c89f5a4f-b55e-4d2b-ec52-4b7ce78bea7a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "./experiments_BD/EGVSR/001\n",
            "2022-09-15 17:20:51,715 [INFO]: ========================================\n",
            "2022-09-15 17:20:51,715 [INFO]: Testing model: EGVSR_iter12000back\n",
            "2022-09-15 17:20:51,715 [INFO]: ========================================\n",
            "2022-09-15 17:20:54,830 [INFO]: Testing on test1: Background\n",
            "No metirc need to compute!\n",
            "torch.Size([15, 540, 960, 3])\n",
            "No metirc need to compute!\n",
            "No metirc need to save!\n",
            "2022-09-15 17:21:19,523 [INFO]: ----------------------------------------\n",
            "2022-09-15 17:21:19,523 [INFO]: Finish testing\n",
            "2022-09-15 17:21:19,523 [INFO]: ========================================\n",
            "--- 27.808297395706177 seconds ---\n",
            "./experiments_BD/EGVSR/001\n",
            "2022-09-15 17:21:21,123 [INFO]: ========================================\n",
            "2022-09-15 17:21:21,123 [INFO]: Testing model: EGVSR_iter12000fore\n",
            "2022-09-15 17:21:21,123 [INFO]: ========================================\n",
            "2022-09-15 17:21:24,317 [INFO]: Testing on test2: Foreground\n",
            "No metirc need to compute!\n",
            "torch.Size([15, 540, 960, 3])\n",
            "No metirc need to compute!\n",
            "No metirc need to save!\n",
            "2022-09-15 17:21:52,720 [INFO]: ----------------------------------------\n",
            "2022-09-15 17:21:52,720 [INFO]: Finish testing\n",
            "2022-09-15 17:21:52,720 [INFO]: ========================================\n",
            "--- 31.597168922424316 seconds ---\n",
            "./experiments_BD/EGVSR/001\n",
            "2022-09-15 17:21:54,325 [INFO]: ========================================\n",
            "2022-09-15 17:21:54,325 [INFO]: Testing model: EGVSR_iter12000\n",
            "2022-09-15 17:21:54,325 [INFO]: ========================================\n",
            "2022-09-15 17:21:57,425 [INFO]: Testing on test2: Full\n",
            "No metirc need to compute!\n",
            "torch.Size([15, 540, 960, 3])\n",
            "No metirc need to compute!\n",
            "No metirc need to save!\n",
            "2022-09-15 17:22:18,121 [INFO]: ----------------------------------------\n",
            "2022-09-15 17:22:18,121 [INFO]: Finish testing\n",
            "2022-09-15 17:22:18,121 [INFO]: ========================================\n",
            "--- 23.795758724212646 seconds ---\n",
            "./experiments_BD/EGVSR/001\n",
            "2022-09-15 17:22:19,406 [INFO]: ========================================\n",
            "2022-09-15 17:22:19,406 [INFO]: Testing model: EGVSR_iter12000back\n",
            "2022-09-15 17:22:19,407 [INFO]: ========================================\n",
            "2022-09-15 17:22:22,471 [INFO]: Testing on test3: FirstMethod\n",
            "No metirc need to compute!\n",
            "torch.Size([15, 540, 960, 3])\n",
            "No metirc need to compute!\n",
            "No metirc need to save!\n",
            "2022-09-15 17:22:57,767 [INFO]: ----------------------------------------\n",
            "2022-09-15 17:22:57,767 [INFO]: Finish testing\n",
            "2022-09-15 17:22:57,767 [INFO]: ========================================\n",
            "--- 38.36085295677185 seconds ---\n",
            "second method finished\n",
            "--- 11.12094521522522 seconds ---\n"
          ]
        }
      ],
      "source": [
        "if best:\n",
        "  !sh test.sh BD EGVSR 002\n",
        "else:\n",
        "  !sh test.sh BD EGVSR 001"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cHKNO_JQSDqR"
      },
      "source": [
        "##Create videos of the resulting frames"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "JOGEif2vO10o"
      },
      "outputs": [],
      "source": [
        "out = cv2.VideoWriter('/content/video/resultFirstMethod.avi',cv2.VideoWriter_fourcc(*'DIVX'), 20, frameSize)\n",
        "if best:\n",
        "  path='/content/results/FirstMethod/EGVSR_iter12000/full/*.png'\n",
        "else:\n",
        "  path='/content/results/FirstMethod/EGVSR_iter12000back/full/*.png'\n",
        "\n",
        "for filename in sorted(glob.glob(path), key=os.path.getmtime):\n",
        "    img = cv2.imread(filename)\n",
        "    out.write(img)\n",
        "\n",
        "out.release()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "O5FO1sC-pwji"
      },
      "outputs": [],
      "source": [
        "out = cv2.VideoWriter('/content/video/resultSecondMethod.avi',cv2.VideoWriter_fourcc(*'DIVX'), 20, frameSize)\n",
        "if best:\n",
        "  path='/content/results/MySecondMod/EGVSR_iter12000/*.png'\n",
        "else:\n",
        "  path='/content/results/MySecondMod/MyG_iter12000/*.png'\n",
        "for filename in sorted(glob.glob(path), key=os.path.getmtime):\n",
        "    img = cv2.imread(filename)\n",
        "    out.write(img)\n",
        "\n",
        "out.release()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c4hvf_O89WIn"
      },
      "source": [
        "#Modules Visualization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O5Ol6MJF9bY3"
      },
      "source": [
        "##Utilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "2jpv95qEw3wK"
      },
      "outputs": [],
      "source": [
        "class BicubicUpsample(nn.Module):\n",
        "\n",
        "    def     __init__(self, scale_factor, a=-0.75):\n",
        "        super(BicubicUpsample, self).__init__()\n",
        "\n",
        "        # calculate weights\n",
        "        cubic = torch.FloatTensor([\n",
        "            [0, a, -2 * a, a],\n",
        "            [1, 0, -(a + 3), a + 2],\n",
        "            [0, -a, (2 * a + 3), -(a + 2)],\n",
        "            [0, 0, a, -a]\n",
        "        ])  # accord to Eq.(6) in the reference paper\n",
        "\n",
        "        kernels = [\n",
        "            torch.matmul(cubic, torch.FloatTensor([1, s, s ** 2, s ** 3]))\n",
        "            for s in [1.0*d/scale_factor for d in range(scale_factor)]\n",
        "        ]  # s = x - floor(x)\n",
        "\n",
        "        # register parameters\n",
        "        self.scale_factor = scale_factor\n",
        "        self.register_buffer('kernels', torch.stack(kernels))\n",
        "\n",
        "    def forward(self, input):\n",
        "        n, c, h, w = input.size()\n",
        "        s = self.scale_factor\n",
        "\n",
        "        # pad input (left, right, top, bottom)\n",
        "        input = F.pad(input, (1, 2, 1, 2), mode='replicate')\n",
        "\n",
        "        # calculate output (height)\n",
        "        kernel_h = self.kernels.repeat(c, 1).view(-1, 1, s, 1)\n",
        "        output = F.conv2d(input, kernel_h, stride=1, padding=0, groups=c)\n",
        "        output = output.reshape(\n",
        "            n, c, s, -1, w + 3).permute(0, 1, 3, 2, 4).reshape(n, c, -1, w + 3)\n",
        "\n",
        "        # calculate output (width)\n",
        "        kernel_w = self.kernels.repeat(c, 1).view(-1, 1, 1, s)\n",
        "        output = F.conv2d(output, kernel_w, stride=1, padding=0, groups=c)\n",
        "        output = output.reshape(\n",
        "            n, c, s, h * s, -1).permute(0, 1, 3, 4, 2).reshape(n, c, h * s, -1)\n",
        "\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "ELLRfc6WxA72"
      },
      "outputs": [],
      "source": [
        "class ResidualBlock(nn.Module):\n",
        "    \"\"\" Residual block without batch normalization\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, nf=64):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(nf, nf, 3, 1, 1, bias=True),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(nf, nf, 3, 1, 1, bias=True))\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv(x) + x\n",
        "\n",
        "        return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "gPPSE1cL7_8y"
      },
      "outputs": [],
      "source": [
        "class BaseSequenceGenerator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(BaseSequenceGenerator, self).__init__()\n",
        "\n",
        "    def generate_dummy_input(self, lr_size):\n",
        "        \"\"\" use for compute per-step FLOPs and speed\n",
        "            return random tensors that can be taken as input of <forward>\n",
        "        \"\"\"\n",
        "        return None\n",
        "\n",
        "    def forward(self, *args, **kwargs):\n",
        "        \"\"\" forward pass for a singe frame\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    def forward_sequence(self, lr_data):\n",
        "        \"\"\" forward pass for a whole sequence (for training)\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    def infer_sequence(self, lr_data, device):\n",
        "        \"\"\" infer for a whole sequence (for inference)\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "\n",
        "class BaseSequenceDiscriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(BaseSequenceDiscriminator, self).__init__()\n",
        "\n",
        "    def forward(self, *args, **kwargs):\n",
        "        \"\"\" forward pass for a singe frame\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    def forward_sequence(self, data, args_dict):\n",
        "        \"\"\" forward pass for a whole sequence (for training)\n",
        "        \"\"\"\n",
        "        pass\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "eKhVOr1a89bt"
      },
      "outputs": [],
      "source": [
        "def get_upsampling_func(scale=4, degradation='BI'):\n",
        "    if degradation == 'BI':\n",
        "        upsample_func = functools.partial(\n",
        "            F.interpolate, scale_factor=scale, mode='bilinear',\n",
        "            align_corners=False)\n",
        "\n",
        "    elif degradation == 'BD':\n",
        "        upsample_func = BicubicUpsample(scale_factor=scale)\n",
        "\n",
        "    else:\n",
        "        raise ValueError('Unrecognized degradation: {}'.format(degradation))\n",
        "\n",
        "    return upsample_func"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wMIQajsl9fnj"
      },
      "source": [
        "##Modules definition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QkoAzjES91ej"
      },
      "source": [
        "###FNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "7OEw59NJ7WOv"
      },
      "outputs": [],
      "source": [
        "class FNet(nn.Module):\n",
        "    \"\"\" Optical flow estimation network\n",
        "    \"\"\"\n",
        "    \n",
        "        \n",
        "    def __init__(self, in_nc):\n",
        "        super().__init__()\n",
        "        super(FNet, self).__init__()\n",
        "\n",
        "        self.encoder1 = nn.Sequential(\n",
        "            nn.Conv2d(2*in_nc, 32, 3, 1, 1, bias=True),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(32, 32, 3, 1, 1, bias=True),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.MaxPool2d(2, 2))\n",
        "\n",
        "        self.encoder2 = nn.Sequential(\n",
        "            nn.Conv2d(32, 64, 3, 1, 1, bias=True),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(64, 64, 3, 1, 1, bias=True),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.MaxPool2d(2, 2))\n",
        "\n",
        "        self.encoder3 = nn.Sequential(\n",
        "            nn.Conv2d(64, 128, 3, 1, 1, bias=True),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(128, 128, 3, 1, 1, bias=True),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.MaxPool2d(2, 2))\n",
        "\n",
        "        self.decoder1 = nn.Sequential(\n",
        "            nn.Conv2d(128, 256, 3, 1, 1, bias=True),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(256, 256, 3, 1, 1, bias=True),\n",
        "            nn.LeakyReLU(0.2, inplace=True))\n",
        "\n",
        "        self.decoder2 = nn.Sequential(\n",
        "            nn.Conv2d(256, 128, 3, 1, 1, bias=True),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(128, 128, 3, 1, 1, bias=True),\n",
        "            nn.LeakyReLU(0.2, inplace=True))\n",
        "\n",
        "        self.decoder3 = nn.Sequential(\n",
        "            nn.Conv2d(128, 64, 3, 1, 1, bias=True),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(64, 64, 3, 1, 1, bias=True),\n",
        "            nn.LeakyReLU(0.2, inplace=True))\n",
        "\n",
        "        self.flow = nn.Sequential(\n",
        "            nn.Conv2d(64, 32, 3, 1, 1, bias=True),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(32, 2, 3, 1, 1, bias=True))\n",
        "\n",
        "    def forward(self, x1, x2):\n",
        "        \"\"\" Compute optical flow from x1 to x2\n",
        "        \"\"\"\n",
        "\n",
        "        out = self.encoder1(torch.cat([x1, x2], dim=1))\n",
        "        out = self.encoder2(out)\n",
        "        out = self.encoder3(out)\n",
        "        out = F.interpolate(\n",
        "            self.decoder1(out), scale_factor=2, mode='bilinear', align_corners=False)\n",
        "        out = F.interpolate(\n",
        "            self.decoder2(out), scale_factor=2, mode='bilinear', align_corners=False)\n",
        "        out = F.interpolate(\n",
        "            self.decoder3(out), scale_factor=2, mode='bilinear', align_corners=False)\n",
        "        out = torch.tanh(self.flow(out)) * 24  # 24 is the max velocity\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-5FG6sjj6kdf",
        "outputId": "d1b558fd-28a5-4e77-cd05-0dbc470efcc6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FNet(\n",
            "  (encoder1): Sequential(\n",
            "    (0): Conv2d(6, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
            "    (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (3): LeakyReLU(negative_slope=0.2, inplace=True)\n",
            "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (encoder2): Sequential(\n",
            "    (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
            "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (3): LeakyReLU(negative_slope=0.2, inplace=True)\n",
            "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (encoder3): Sequential(\n",
            "    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
            "    (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (3): LeakyReLU(negative_slope=0.2, inplace=True)\n",
            "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (decoder1): Sequential(\n",
            "    (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
            "    (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (3): LeakyReLU(negative_slope=0.2, inplace=True)\n",
            "  )\n",
            "  (decoder2): Sequential(\n",
            "    (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
            "    (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (3): LeakyReLU(negative_slope=0.2, inplace=True)\n",
            "  )\n",
            "  (decoder3): Sequential(\n",
            "    (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
            "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (3): LeakyReLU(negative_slope=0.2, inplace=True)\n",
            "  )\n",
            "  (flow): Sequential(\n",
            "    (0): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
            "    (2): Conv2d(32, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "fnet=FNet(3)\n",
        "print(fnet)\n",
        "with open(\"fnet.txt\", \"a\") as f:\n",
        "  print(fnet, file=f)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xy5Vml1I9_f_"
      },
      "source": [
        "###SRNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "oqDa1e1QwN7C"
      },
      "outputs": [],
      "source": [
        "class SRNet(nn.Module):\n",
        "    \"\"\" Reconstruction & Upsampling network\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_nc=3, out_nc=3, nf=64, nb=16, upsample_func=None,\n",
        "                 scale=4):\n",
        "        super(SRNet, self).__init__()\n",
        "\n",
        "        # input conv.\n",
        "        self.conv_in = nn.Sequential(\n",
        "            nn.Conv2d((scale**2 + 1) * in_nc, nf, 3, 1, 1, bias=True),\n",
        "            nn.ReLU(inplace=True))\n",
        "\n",
        "        # residual blocks\n",
        "        self.resblocks = nn.Sequential(*[ResidualBlock(nf) for _ in range(nb)])\n",
        "\n",
        "        # upsampling\n",
        "        self.conv_up = nn.Sequential(\n",
        "            nn.ConvTranspose2d(nf, nf, 3, 2, 1, output_padding=1, bias=True),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.ConvTranspose2d(nf, nf, 3, 2, 1, output_padding=1, bias=True),\n",
        "            nn.ReLU(inplace=True))\n",
        "\n",
        "        self.conv_up_cheap = nn.Sequential(\n",
        "            nn.PixelShuffle(4),\n",
        "            nn.ReLU(inplace=True))\n",
        "\n",
        "        # output conv.\n",
        "        self.conv_out = nn.Conv2d(4, out_nc, 3, 1, 1, bias=True)\n",
        "\n",
        "        # upsampling function\n",
        "        self.upsample_func = upsample_func\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\" lr_curr: the current lr data in shape nchw\n",
        "            hr_prev_tran: the previous transformed hr_data in shape n(4*4*c)hw\n",
        "        \"\"\"\n",
        "\n",
        "        out = self.conv_in(x)\n",
        "        out = self.resblocks(out)\n",
        "        out = self.conv_up_cheap(out)\n",
        "        out = self.conv_out(out)\n",
        "        # out += self.upsample_func(lr_curr)\n",
        "\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v443Bhtaw6W4",
        "outputId": "01830a7f-728a-4bfa-ee03-cec1d5be6afa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SRNet(\n",
            "  (conv_in): Sequential(\n",
            "    (0): Conv2d(51, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): ReLU(inplace=True)\n",
            "  )\n",
            "  (resblocks): Sequential(\n",
            "    (0): ResidualBlock(\n",
            "      (conv): Sequential(\n",
            "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (1): ReLU(inplace=True)\n",
            "        (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      )\n",
            "    )\n",
            "    (1): ResidualBlock(\n",
            "      (conv): Sequential(\n",
            "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (1): ReLU(inplace=True)\n",
            "        (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      )\n",
            "    )\n",
            "    (2): ResidualBlock(\n",
            "      (conv): Sequential(\n",
            "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (1): ReLU(inplace=True)\n",
            "        (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      )\n",
            "    )\n",
            "    (3): ResidualBlock(\n",
            "      (conv): Sequential(\n",
            "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (1): ReLU(inplace=True)\n",
            "        (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      )\n",
            "    )\n",
            "    (4): ResidualBlock(\n",
            "      (conv): Sequential(\n",
            "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (1): ReLU(inplace=True)\n",
            "        (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      )\n",
            "    )\n",
            "    (5): ResidualBlock(\n",
            "      (conv): Sequential(\n",
            "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (1): ReLU(inplace=True)\n",
            "        (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      )\n",
            "    )\n",
            "    (6): ResidualBlock(\n",
            "      (conv): Sequential(\n",
            "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (1): ReLU(inplace=True)\n",
            "        (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      )\n",
            "    )\n",
            "    (7): ResidualBlock(\n",
            "      (conv): Sequential(\n",
            "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (1): ReLU(inplace=True)\n",
            "        (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      )\n",
            "    )\n",
            "    (8): ResidualBlock(\n",
            "      (conv): Sequential(\n",
            "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (1): ReLU(inplace=True)\n",
            "        (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      )\n",
            "    )\n",
            "    (9): ResidualBlock(\n",
            "      (conv): Sequential(\n",
            "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (1): ReLU(inplace=True)\n",
            "        (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      )\n",
            "    )\n",
            "    (10): ResidualBlock(\n",
            "      (conv): Sequential(\n",
            "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (1): ReLU(inplace=True)\n",
            "        (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      )\n",
            "    )\n",
            "    (11): ResidualBlock(\n",
            "      (conv): Sequential(\n",
            "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (1): ReLU(inplace=True)\n",
            "        (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      )\n",
            "    )\n",
            "    (12): ResidualBlock(\n",
            "      (conv): Sequential(\n",
            "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (1): ReLU(inplace=True)\n",
            "        (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      )\n",
            "    )\n",
            "    (13): ResidualBlock(\n",
            "      (conv): Sequential(\n",
            "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (1): ReLU(inplace=True)\n",
            "        (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      )\n",
            "    )\n",
            "    (14): ResidualBlock(\n",
            "      (conv): Sequential(\n",
            "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (1): ReLU(inplace=True)\n",
            "        (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      )\n",
            "    )\n",
            "    (15): ResidualBlock(\n",
            "      (conv): Sequential(\n",
            "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (1): ReLU(inplace=True)\n",
            "        (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (conv_up): Sequential(\n",
            "    (0): ConvTranspose2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
            "    (1): ReLU(inplace=True)\n",
            "    (2): ConvTranspose2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
            "    (3): ReLU(inplace=True)\n",
            "  )\n",
            "  (conv_up_cheap): Sequential(\n",
            "    (0): PixelShuffle(upscale_factor=4)\n",
            "    (1): ReLU(inplace=True)\n",
            "  )\n",
            "  (conv_out): Conv2d(4, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "srnet = SRNet()\n",
        "print(srnet)\n",
        "with open(\"srnet.txt\", \"a\") as f:\n",
        "  print(srnet, file=f)\n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w_7TsiV_-P5n"
      },
      "source": [
        "###FRNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "Azs_ecE-70yH"
      },
      "outputs": [],
      "source": [
        "class FRNet(BaseSequenceGenerator):\n",
        "    \"\"\" Frame-recurrent network proposed in https://arxiv.org/abs/1801.04590\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_nc=3, out_nc=3, nf=64, nb=16, degradation='BI',\n",
        "                 scale=4):\n",
        "        super(FRNet, self).__init__()\n",
        "\n",
        "        self.scale = scale\n",
        "\n",
        "        # get upsampling function according to the degradation mode\n",
        "        self.upsample_func = get_upsampling_func(self.scale, degradation)\n",
        "\n",
        "        # define fnet & srnet\n",
        "        self.fnet = FNet(in_nc)\n",
        "        self.srnet = SRNet(in_nc, out_nc, nf, nb, self.upsample_func)\n",
        "\n",
        "    def generate_dummy_input(self, lr_size):\n",
        "        c, lr_h, lr_w = lr_size\n",
        "        s = self.scale\n",
        "\n",
        "        lr_curr = torch.rand(1, c, lr_h, lr_w, dtype=torch.float32)\n",
        "        lr_prev = torch.rand(1, c, lr_h, lr_w, dtype=torch.float32)\n",
        "        hr_prev = torch.rand(1, c, s * lr_h, s * lr_w, dtype=torch.float32)\n",
        "\n",
        "        data_dict = {\n",
        "            'lr_curr': lr_curr,\n",
        "            'lr_prev': lr_prev,\n",
        "            'hr_prev': hr_prev\n",
        "        }\n",
        "\n",
        "        return data_dict\n",
        "\n",
        "    def forward(self, lr_curr, lr_prev, hr_prev):\n",
        "        \"\"\"\n",
        "            Parameters:\n",
        "                :param lr_curr: the current lr data in shape nchw\n",
        "                :param lr_prev: the previous lr data in shape nchw\n",
        "                :param hr_prev: the previous hr data in shape nc(4h)(4w)\n",
        "        \"\"\"\n",
        "\n",
        "        # estimate lr flow (lr_curr -> lr_prev)\n",
        "        lr_flow = self.fnet(lr_curr, lr_prev)\n",
        "\n",
        "        # pad if size is not a multiple of 8\n",
        "        pad_h = lr_curr.size(2) - lr_curr.size(2) // 8 * 8\n",
        "        pad_w = lr_curr.size(3) - lr_curr.size(3) // 8 * 8\n",
        "        lr_flow_pad = F.pad(lr_flow, (0, pad_w, 0, pad_h), 'reflect')\n",
        "\n",
        "        # upsample lr flow\n",
        "        hr_flow = self.scale * self.upsample_func(lr_flow_pad)\n",
        "\n",
        "        # warp hr_prev\n",
        "        hr_prev_warp = backward_warp(hr_prev, hr_flow)\n",
        "\n",
        "        # compute hr_curr\n",
        "        hr_curr = self.srnet(lr_curr, space_to_depth(hr_prev_warp, self.scale))\n",
        "        concres= torch.cat([lr_curr, space_to_depth(hr_prev_warp)],1)\n",
        "        return hr_curr,concres\n",
        "\n",
        "    def forward_sequence(self, lr_data):\n",
        "        \"\"\"\n",
        "            Parameters:\n",
        "                :param lr_data: lr data in shape ntchw\n",
        "        \"\"\"\n",
        "\n",
        "        n, t, c, lr_h, lr_w = lr_data.size()\n",
        "        hr_h, hr_w = lr_h * self.scale, lr_w * self.scale\n",
        "\n",
        "        # calculate optical flows\n",
        "        lr_prev = lr_data[:, :-1, ...].reshape(n * (t - 1), c, lr_h, lr_w)\n",
        "        lr_curr = lr_data[:, 1:, ...].reshape(n * (t - 1), c, lr_h, lr_w)\n",
        "        file_name = \"sample.pkl\"\n",
        "\n",
        "        open_file = open(file_name, \"wb\")\n",
        "        pickle.dump(lr_curr, open_file)\n",
        "        open_file.close()\n",
        "        lr_flow = self.fnet(lr_curr, lr_prev)  # n*(t-1),2,h,w\n",
        "\n",
        "        # upsample lr flows\n",
        "        hr_flow = self.scale * self.upsample_func(lr_flow)\n",
        "        hr_flow = hr_flow.view(n, (t - 1), 2, hr_h, hr_w)\n",
        "\n",
        "        # compute the first hr data\n",
        "        hr_data = []\n",
        "        hr_prev = self.srnet(\n",
        "            lr_data[:, 0, ...],\n",
        "            torch.zeros(n, (self.scale**2)*c, lr_h, lr_w, dtype=torch.float32,\n",
        "                        device=lr_data.device))\n",
        "        hr_data.append(hr_prev)\n",
        "\n",
        "        # compute the remaining hr data\n",
        "        for i in range(1, t):\n",
        "            # warp hr_prev\n",
        "            hr_prev_warp = backward_warp(hr_prev, hr_flow[:, i - 1, ...])\n",
        "\n",
        "            # compute hr_curr\n",
        "            hr_curr = self.srnet(\n",
        "                lr_data[:, i, ...],\n",
        "                space_to_depth(hr_prev_warp, self.scale))\n",
        "\n",
        "            # save and update\n",
        "            hr_data.append(hr_curr)\n",
        "            hr_prev = hr_curr\n",
        "\n",
        "        hr_data = torch.stack(hr_data, dim=1)  # n,t,c,hr_h,hr_w\n",
        "\n",
        "        # construct output dict\n",
        "        ret_dict = {\n",
        "            'hr_data': hr_data,  # n,t,c,hr_h,hr_w\n",
        "            'hr_flow': hr_flow,  # n,t,2,hr_h,hr_w\n",
        "            'lr_prev': lr_prev,  # n(t-1),c,lr_h,lr_w\n",
        "            'lr_curr': lr_curr,  # n(t-1),c,lr_h,lr_w\n",
        "            'lr_flow': lr_flow,  # n(t-1),2,lr_h,lr_w\n",
        "        }\n",
        "\n",
        "        return ret_dict\n",
        "\n",
        "    def infer_sequence(self, lr_data, device):\n",
        "        \"\"\"\n",
        "            Parameters:\n",
        "                :param lr_data: torch.FloatTensor in shape tchw\n",
        "                :param device: torch.device\n",
        "\n",
        "                :return hr_seq: uint8 np.ndarray in shape tchw\n",
        "        \"\"\"\n",
        "\n",
        "        # setup params\n",
        "        tot_frm, c, h, w = lr_data.size()\n",
        "        s = self.scale\n",
        "\n",
        "        # forward\n",
        "        hr_seq = []\n",
        "        lr_prev = torch.zeros(1, c, h, w, dtype=torch.float32).to(device)\n",
        "        hr_prev = torch.zeros(\n",
        "            1, c, s * h, s * w, dtype=torch.float32).to(device)\n",
        "        tensorlist = []\n",
        "        for i in range(tot_frm):\n",
        "            with torch.no_grad():\n",
        "                self.eval()\n",
        "\n",
        "                lr_curr = lr_data[i: i + 1, ...].to(device)\n",
        "                hr_curr,concres = self.forward(lr_curr, lr_prev, hr_prev) ##la domanda Ã¨ cosa fare con questo concres? che Ã¨ un tensore attualmente\n",
        "                lr_prev, hr_prev = lr_curr, hr_curr\n",
        "                tensorlist.append(concres) #######non so se devo prima fare queeze in numpy\n",
        "                hr_frm = hr_curr.squeeze(0).cpu().numpy()  # chw|rgb|uint8\n",
        "\n",
        "            hr_seq.append(float32_to_uint8(hr_frm))\n",
        "        file_name = \"sample.pkl\"\n",
        "\n",
        "        open_file = open(file_name, \"wb\")\n",
        "        pickle.dump(tensorlist, open_file)\n",
        "        open_file.close()\n",
        "\n",
        "        return np.stack(hr_seq).transpose(0, 2, 3, 1)  # thwc\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Btyybo358DJ0",
        "outputId": "647314fd-4089-4759-ae68-d470b5770c0a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FRNet(\n",
            "  (fnet): FNet(\n",
            "    (encoder1): Sequential(\n",
            "      (0): Conv2d(6, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
            "      (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (3): LeakyReLU(negative_slope=0.2, inplace=True)\n",
            "      (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    )\n",
            "    (encoder2): Sequential(\n",
            "      (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
            "      (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (3): LeakyReLU(negative_slope=0.2, inplace=True)\n",
            "      (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    )\n",
            "    (encoder3): Sequential(\n",
            "      (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
            "      (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (3): LeakyReLU(negative_slope=0.2, inplace=True)\n",
            "      (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    )\n",
            "    (decoder1): Sequential(\n",
            "      (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
            "      (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (3): LeakyReLU(negative_slope=0.2, inplace=True)\n",
            "    )\n",
            "    (decoder2): Sequential(\n",
            "      (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
            "      (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (3): LeakyReLU(negative_slope=0.2, inplace=True)\n",
            "    )\n",
            "    (decoder3): Sequential(\n",
            "      (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
            "      (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (3): LeakyReLU(negative_slope=0.2, inplace=True)\n",
            "    )\n",
            "    (flow): Sequential(\n",
            "      (0): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
            "      (2): Conv2d(32, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    )\n",
            "  )\n",
            "  (srnet): SRNet(\n",
            "    (conv_in): Sequential(\n",
            "      (0): Conv2d(51, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (1): ReLU(inplace=True)\n",
            "    )\n",
            "    (resblocks): Sequential(\n",
            "      (0): ResidualBlock(\n",
            "        (conv): Sequential(\n",
            "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          (1): ReLU(inplace=True)\n",
            "          (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        )\n",
            "      )\n",
            "      (1): ResidualBlock(\n",
            "        (conv): Sequential(\n",
            "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          (1): ReLU(inplace=True)\n",
            "          (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        )\n",
            "      )\n",
            "      (2): ResidualBlock(\n",
            "        (conv): Sequential(\n",
            "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          (1): ReLU(inplace=True)\n",
            "          (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        )\n",
            "      )\n",
            "      (3): ResidualBlock(\n",
            "        (conv): Sequential(\n",
            "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          (1): ReLU(inplace=True)\n",
            "          (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        )\n",
            "      )\n",
            "      (4): ResidualBlock(\n",
            "        (conv): Sequential(\n",
            "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          (1): ReLU(inplace=True)\n",
            "          (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        )\n",
            "      )\n",
            "      (5): ResidualBlock(\n",
            "        (conv): Sequential(\n",
            "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          (1): ReLU(inplace=True)\n",
            "          (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        )\n",
            "      )\n",
            "      (6): ResidualBlock(\n",
            "        (conv): Sequential(\n",
            "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          (1): ReLU(inplace=True)\n",
            "          (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        )\n",
            "      )\n",
            "      (7): ResidualBlock(\n",
            "        (conv): Sequential(\n",
            "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          (1): ReLU(inplace=True)\n",
            "          (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        )\n",
            "      )\n",
            "      (8): ResidualBlock(\n",
            "        (conv): Sequential(\n",
            "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          (1): ReLU(inplace=True)\n",
            "          (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        )\n",
            "      )\n",
            "      (9): ResidualBlock(\n",
            "        (conv): Sequential(\n",
            "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          (1): ReLU(inplace=True)\n",
            "          (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        )\n",
            "      )\n",
            "      (10): ResidualBlock(\n",
            "        (conv): Sequential(\n",
            "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          (1): ReLU(inplace=True)\n",
            "          (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        )\n",
            "      )\n",
            "      (11): ResidualBlock(\n",
            "        (conv): Sequential(\n",
            "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          (1): ReLU(inplace=True)\n",
            "          (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        )\n",
            "      )\n",
            "      (12): ResidualBlock(\n",
            "        (conv): Sequential(\n",
            "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          (1): ReLU(inplace=True)\n",
            "          (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        )\n",
            "      )\n",
            "      (13): ResidualBlock(\n",
            "        (conv): Sequential(\n",
            "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          (1): ReLU(inplace=True)\n",
            "          (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        )\n",
            "      )\n",
            "      (14): ResidualBlock(\n",
            "        (conv): Sequential(\n",
            "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          (1): ReLU(inplace=True)\n",
            "          (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        )\n",
            "      )\n",
            "      (15): ResidualBlock(\n",
            "        (conv): Sequential(\n",
            "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          (1): ReLU(inplace=True)\n",
            "          (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (conv_up): Sequential(\n",
            "      (0): ConvTranspose2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
            "      (1): ReLU(inplace=True)\n",
            "      (2): ConvTranspose2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
            "      (3): ReLU(inplace=True)\n",
            "    )\n",
            "    (conv_up_cheap): Sequential(\n",
            "      (0): PixelShuffle(upscale_factor=4)\n",
            "      (1): ReLU(inplace=True)\n",
            "    )\n",
            "    (conv_out): Conv2d(4, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "frnet=FRNet()\n",
        "print(frnet)\n",
        "with open(\"frnet.txt\", \"a\") as f:\n",
        "  print(frnet, file=f)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X3Qkq-azfrjY"
      },
      "source": [
        "#Result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T0wH_ZPZhHVM",
        "outputId": "32ee2d4f-9df8-48f8-8b4b-200dbeebc74b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ],
      "source": [
        "%cd /content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2bOZ3-ZsS7Ue"
      },
      "source": [
        "clonation of the code for PerceptualSimilarity measurements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OIT9pG2X9gvj",
        "outputId": "f7ef7ff2-9174-40a3-cee9-78cc1dac4224"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'PerceptualSimilarity'...\n",
            "remote: Enumerating objects: 778, done.\u001b[K\n",
            "remote: Counting objects: 100% (775/775), done.\u001b[K\n",
            "remote: Compressing objects: 100% (316/316), done.\u001b[K\n",
            "remote: Total 778 (delta 457), reused 774 (delta 456), pack-reused 3\u001b[K\n",
            "Receiving objects: 100% (778/778), 8.75 MiB | 29.29 MiB/s, done.\n",
            "Resolving deltas: 100% (457/457), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/richzhang/PerceptualSimilarity\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HZxVM8DjhRgE"
      },
      "source": [
        "##Methods definition of the metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iuuRF_ar9qlj",
        "outputId": "1bea8a5e-977f-4079-e7ce-f2374c397bc0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/PerceptualSimilarity\n"
          ]
        }
      ],
      "source": [
        "%cd PerceptualSimilarity\n",
        "import lpips"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "BBear6Ib0Xcm"
      },
      "outputs": [],
      "source": [
        "def PSNR(path1,path2,mod=''):\n",
        "  PSNR=[]\n",
        "  PSNR1=[]\n",
        "  if mod=='baseline':    \n",
        "    list_of_files = filter( os.path.isfile,\n",
        "                        glob.glob(path1 + '*'))\n",
        "    list_of_files = sorted( list_of_files,\n",
        "                        key = os.path.getmtime)\n",
        "    list_of_files2 = filter( os.path.isfile,\n",
        "                        glob.glob(path2 + '*'))\n",
        "    list_of_files2 = sorted(list_of_files2,key = os.path.getmtime)   \n",
        "    j=0\n",
        "    for image in sorted( list_of_files,key = os.path.getmtime):      \n",
        "      f = cv2.imread(image)\n",
        "      g = cv2.imread(list_of_files2[j], 1)\n",
        "      psnr1= cv2.PSNR(f, g)\n",
        "      PSNR1.append(psnr1)\n",
        "      results = yolo(f)    \n",
        "      A=results.xyxyn[0].data.cpu().numpy()\n",
        "      j+=1    \n",
        "      for i in range(A.shape[0]):\n",
        "          c=np.array(f[int(A[i][1]*(f.shape[0])):int(A[i][3]*(f.shape[0])),int(A[i][0]*(f.shape[1])):int(A[i][2]*(f.shape[1]))])\n",
        "          d=np.array(g[int(A[i][1]*(f.shape[0])):int(A[i][3]*(f.shape[0])),int(A[i][0]*(f.shape[1])):int(A[i][2]*(f.shape[1]))]) \n",
        "          psnr= cv2.PSNR(c, d)\n",
        "          PSNR.append(psnr)\n",
        "  else:\n",
        "    images1 = [file for file in os.listdir(path1) if file.endswith(('png'))]  \n",
        "    images2 = [file for file in os.listdir(path2) if file.endswith(('png'))]\n",
        "    \n",
        "    for i in range(len(images1)):       \n",
        "        f = cv2.imread(path1+'/'+images1[i])\n",
        "        g = cv2.imread(path2+'/'+images2[i], 1)\n",
        "        psnr1= cv2.PSNR(f, g)\n",
        "        PSNR1.append(psnr1)\n",
        "        results = yolo(f)    \n",
        "        A=results.xyxyn[0].data.cpu().numpy()    \n",
        "        for i in range(A.shape[0]):\n",
        "            c=np.array(f[int(A[i][1]*(f.shape[0])):int(A[i][3]*(f.shape[0])),int(A[i][0]*(f.shape[1])):int(A[i][2]*(f.shape[1]))])\n",
        "            d=np.array(g[int(A[i][1]*(f.shape[0])):int(A[i][3]*(f.shape[0])),int(A[i][0]*(f.shape[1])):int(A[i][2]*(f.shape[1]))]) \n",
        "            psnr= cv2.PSNR(c, d)\n",
        "            PSNR.append(psnr)     \n",
        "  psnrpatch=np.sum(PSNR)/len(PSNR)\n",
        "  print(\"PSNR calcolato su patch Ã¨: \" + str(psnrpatch))\n",
        "  psnrfull=np.sum(PSNR1)/len(PSNR1)  \n",
        "  print(\"PSNR calcolato intera immagine Ã¨: \" + str(psnrfull));\n",
        "  return psnr,psnrfull"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "8G97WlPY4mHD"
      },
      "outputs": [],
      "source": [
        "def mse(imageA, imageB):\n",
        " # the 'Mean Squared Error' between the two images is the sum of the squared difference between the two images\n",
        " mse_error = np.sum((imageA.astype(\"float\") - imageB.astype(\"float\")) ** 2)\n",
        " mse_error /= float(imageA.shape[0] * imageA.shape[1])\n",
        "\t\n",
        " # return the MSE. The lower the error, the more \"similar\" the two images are.\n",
        " return mse_error\n",
        "\n",
        "def compare(imageA, imageB):\n",
        " # Calculate the MSE and SSIM\n",
        " m = mse(imageA, imageB)\n",
        " s = ssim(imageA, imageB)\n",
        " return s"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "2IovtwzA3ovS"
      },
      "outputs": [],
      "source": [
        "def SSIM(path1,path2,mod=''):  \n",
        "  SSIM=[]\n",
        "  MSE=[]\n",
        "  SSIMPatch=[]\n",
        "  MSEPatch=[]\n",
        "  PSNR=[]\n",
        "  PSNR1=[]\n",
        "  if mod=='baseline':    \n",
        "    list_of_files = filter( os.path.isfile,\n",
        "                        glob.glob(path1 + '*'))\n",
        "    list_of_files = sorted( list_of_files,\n",
        "                        key = os.path.getmtime)\n",
        "    list_of_files2 = filter( os.path.isfile,\n",
        "                        glob.glob(path2 + '*'))\n",
        "    list_of_files2 = sorted(list_of_files2,key = os.path.getmtime)   \n",
        "    j=0\n",
        "    for image in sorted( list_of_files,key = os.path.getmtime): \n",
        "        f = cv2.imread(image)\n",
        "        g = cv2.imread(list_of_files2[j], 1)\n",
        "        gray1 = cv2.cvtColor(f, cv2.COLOR_BGR2GRAY)\n",
        "        gray2 = cv2.cvtColor(g, cv2.COLOR_BGR2GRAY)    \n",
        "        mse_value = mse(gray1, gray2)\n",
        "        ssim_value = compare(gray1, gray2)\n",
        "        SSIM.append(ssim_value)\n",
        "        MSE.append(mse_value)\n",
        "        results = yolo(f)    \n",
        "        A=results.xyxyn[0].data.cpu().numpy()\n",
        "        j+=1         \n",
        "        for i in range(A.shape[0]):\n",
        "          c=np.array(gray1[int(A[i][1]*(f.shape[0])):int(A[i][3]*(f.shape[0])),int(A[i][0]*(f.shape[1])):int(A[i][2]*(f.shape[1]))])\n",
        "          d=np.array(gray2[int(A[i][1]*(f.shape[0])):int(A[i][3]*(f.shape[0])),int(A[i][0]*(f.shape[1])):int(A[i][2]*(f.shape[1]))]) \n",
        "          mse_valuepatch = mse(c, d)\n",
        "          ssim_valuepatch = compare(c, d)\n",
        "          SSIMPatch.append(ssim_valuepatch)\n",
        "          MSEPatch.append(mse_valuepatch)\n",
        "  \n",
        "  else:\n",
        "    images1 = [file for file in os.listdir(path1) if file.endswith(('png'))]\n",
        "    images2 = [file for file in os.listdir(path2) if file.endswith(('png'))]\n",
        "    for i in range(len(images1)):\n",
        "        f = cv2.imread(path1+'/'+images1[i])\n",
        "        g = cv2.imread(path2+'/'+images2[i], 1)\n",
        "        gray1 = cv2.cvtColor(f, cv2.COLOR_BGR2GRAY)\n",
        "        gray2 = cv2.cvtColor(g, cv2.COLOR_BGR2GRAY)    \n",
        "        mse_value = mse(gray1, gray2)\n",
        "        ssim_value = compare(gray1, gray2)\n",
        "        SSIM.append(ssim_value)\n",
        "        MSE.append(mse_value)\n",
        "        results = yolo(f)    \n",
        "        A=results.xyxyn[0].data.cpu().numpy()         \n",
        "        for i in range(A.shape[0]):\n",
        "          c=np.array(gray1[int(A[i][1]*(f.shape[0])):int(A[i][3]*(f.shape[0])),int(A[i][0]*(f.shape[1])):int(A[i][2]*(f.shape[1]))])\n",
        "          d=np.array(gray2[int(A[i][1]*(f.shape[0])):int(A[i][3]*(f.shape[0])),int(A[i][0]*(f.shape[1])):int(A[i][2]*(f.shape[1]))]) \n",
        "          mse_valuepatch = mse(c, d)\n",
        "          ssim_valuepatch = compare(c, d)\n",
        "          SSIMPatch.append(ssim_valuepatch)\n",
        "          MSEPatch.append(mse_valuepatch)\n",
        "  \n",
        "  ssimtot=np.sum(SSIM)/len(SSIM)\n",
        "  print(\"SSIM calcolato su intere immagini Ã¨: \" + str(ssimtot))  \n",
        "   \n",
        "  msetot=np.sum(MSE)/len(MSE)\n",
        "  #print(\"MSE calcolato su intere immagini Ã¨: \" + str(msetot))\n",
        "  \n",
        "  ssimtotpatch=np.sum(SSIMPatch)/len(SSIMPatch)\n",
        "  print(\"SSIM calcolato su Patch Ã¨: \" + str(ssimtotpatch));         \n",
        "  msetotpatch=np.sum(MSEPatch)/len(MSEPatch)\n",
        "  #print(\"MSE calcolato su Patch Ã¨: \" + str(msetotpatch));   \n",
        "  return ssimtot,msetot,ssimtotpatch,msetotpatch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "7yAHfBjk84Nu"
      },
      "outputs": [],
      "source": [
        "def DIST(path1,path2,mod=''):  \n",
        "  \n",
        "  DIST=[]\n",
        "  DISTPatch=[]\n",
        "  loss_fn = lpips.LPIPS(net='alex')\n",
        "  loss_fn.cuda()\n",
        "\n",
        "  if mod=='baseline':    \n",
        "    list_of_files = filter( os.path.isfile,\n",
        "                        glob.glob(path1 + '*'))\n",
        "    list_of_files = sorted( list_of_files,\n",
        "                        key = os.path.getmtime)\n",
        "    list_of_files2 = filter( os.path.isfile,\n",
        "                        glob.glob(path2 + '*'))\n",
        "    list_of_files2 = sorted(list_of_files2,key = os.path.getmtime)   \n",
        "    j=0\n",
        "    for image in sorted( list_of_files,key = os.path.getmtime):\n",
        "      f = cv2.imread(image)\n",
        "      g = cv2.imread(list_of_files2[j], 1)\n",
        "      results = yolo(f)    \n",
        "      A=results.xyxyn[0].data.cpu().numpy()\n",
        "      j+=1 \n",
        "      for i in range(A.shape[0]-1):\n",
        "          c=np.array(f[int(A[i][1]*(f.shape[0])):int(A[i][3]*(f.shape[0])),int(A[i][0]*(f.shape[1])):int(A[i][2]*(f.shape[1]))])\n",
        "          d=np.array(g[int(A[i][1]*(f.shape[0])):int(A[i][3]*(f.shape[0])),int(A[i][0]*(f.shape[1])):int(A[i][2]*(f.shape[1]))])\n",
        "          \n",
        "          h=(c / 255./2. - 1)[:, :, :, np.newaxis].transpose((3, 2, 0, 1))\n",
        "          hh=(d / 255./2. - 1)[:, :, :, np.newaxis].transpose((3, 2, 0, 1))\n",
        "          h=torch.Tensor(h)\n",
        "          hh=torch.Tensor(hh)\n",
        "          h = h.cuda()\n",
        "          hh = hh.cuda()\n",
        "          dist02 = loss_fn.forward(h,hh)\n",
        "          DISTPatch.append(dist02[0][0][0][0].cpu().detach().numpy())\n",
        "          torch.cuda.empty_cache()  \n",
        "     \n",
        "      k=(f / 255./2. - 1)[:, :, :, np.newaxis].transpose((3, 2, 0, 1))\n",
        "      kk=(g / 255./2. - 1)[:, :, :, np.newaxis].transpose((3, 2, 0, 1))\n",
        "      k=torch.Tensor(k)\n",
        "      kk=torch.Tensor(kk)\n",
        "      k = k.cuda()\n",
        "      kk = kk.cuda()\n",
        "      dist01 = loss_fn.forward(k,kk)\n",
        "      DIST.append(dist01[0][0][0][0].cpu().detach().numpy())\n",
        "      torch.cuda.empty_cache()\n",
        "  else:    \n",
        "\n",
        "    images1 = [file for file in os.listdir(path1) if file.endswith(('png'))]  \n",
        "    images2 = [file for file in os.listdir(path2) if file.endswith(('png'))]\n",
        "    for i in range(len(images1)):\n",
        "        f = cv2.imread(path1+'/'+images1[i])\n",
        "        g = cv2.imread(path2+'/'+images2[i], 1)\n",
        "        results = yolo(f)    \n",
        "        A=results.xyxyn[0].data.cpu().numpy()\n",
        "        for i in range(A.shape[0]-1):\n",
        "            c=np.array(f[int(A[i][1]*(f.shape[0])):int(A[i][3]*(f.shape[0])),int(A[i][0]*(f.shape[1])):int(A[i][2]*(f.shape[1]))])\n",
        "            d=np.array(g[int(A[i][1]*(f.shape[0])):int(A[i][3]*(f.shape[0])),int(A[i][0]*(f.shape[1])):int(A[i][2]*(f.shape[1]))])\n",
        "            \n",
        "            h=(c / 255./2. - 1)[:, :, :, np.newaxis].transpose((3, 2, 0, 1))\n",
        "            hh=(d / 255./2. - 1)[:, :, :, np.newaxis].transpose((3, 2, 0, 1))\n",
        "            h=torch.Tensor(h)\n",
        "            hh=torch.Tensor(hh)\n",
        "            h = h.cuda()\n",
        "            hh = hh.cuda()\n",
        "            dist02 = loss_fn.forward(h,hh)\n",
        "            DISTPatch.append(dist02[0][0][0][0].cpu().detach().numpy())\n",
        "            torch.cuda.empty_cache()          \n",
        "        k=(f / 255./2. - 1)[:, :, :, np.newaxis].transpose((3, 2, 0, 1))\n",
        "        kk=(g / 255./2. - 1)[:, :, :, np.newaxis].transpose((3, 2, 0, 1))\n",
        "        k=torch.Tensor(k)\n",
        "        kk=torch.Tensor(kk)\n",
        "        k = k.cuda()\n",
        "        kk = kk.cuda()\n",
        "        dist01 = loss_fn.forward(k,kk)\n",
        "        DIST.append(dist01[0][0][0][0].cpu().detach().numpy())\n",
        "        torch.cuda.empty_cache()\n",
        "        \n",
        "  dist1=np.sum(DIST)/len(DIST)  \n",
        "  print(\"Distance su tutta immagine : \" + str(dist1))\n",
        "  distpetch=np.sum(DISTPatch)/len(DISTPatch)  \n",
        "  print(\"Distance su patch : \" + str(distpetch))\n",
        "  return dist1 ,distpetch "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KNJ52_hqh2Ov"
      },
      "source": [
        "##Call methods"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3zVRBPQVvjPb"
      },
      "source": [
        "###Results First Method"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if best:\n",
        "  pathFirstMod='/content/results/FirstMethod/EGVSR_iter12000/full/'\n",
        "else:\n",
        "  pathFirstMod='/content/results/FirstMethod/EGVSR_iter12000back/full/'"
      ],
      "metadata": {
        "id": "yRr3l8KoHnLv"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9d2QS5fL0uD4",
        "outputId": "d0c92890-57b3-4eba-f264-555ab2d01b41"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PSNR calcolato su patch Ã¨: 25.192784819883457\n",
            "PSNR calcolato intera immagine Ã¨: 29.60457198583706\n",
            "PSNR calcolato su patch Ã¨: 25.034947827665075\n",
            "PSNR calcolato intera immagine Ã¨: 29.568866952211216\n"
          ]
        }
      ],
      "source": [
        "PSNRBaselinepatch,baselineFull=PSNR('/content/dataset/full/GT/full/','/content/results/Full/EGVSR_iter12000/full/',mod='baseline')\n",
        "PSNRMyMod,MyModFull=PSNR('/content/dataset/full/GT/full/',pathFirstMod,mod='baseline')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k6AACfNw5Drd",
        "outputId": "bd3beeed-214e-4c08-c4ae-a26504fa7662"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SSIM calcolato su intere immagini Ã¨: 0.9398859157592359\n",
            "SSIM calcolato su Patch Ã¨: 0.8416278529907878\n",
            "SSIM calcolato su intere immagini Ã¨: 0.9370272001899025\n",
            "SSIM calcolato su Patch Ã¨: 0.8285968752973343\n"
          ]
        }
      ],
      "source": [
        "SSIMBaselineFull,MSEbaselineFull,SSIMBaselinepatch,MSEbaselinepatch=SSIM('/content/dataset/full/GT/full/','/content/results/Full/EGVSR_iter12000/full/',mod='baseline')\n",
        "SSIMMymodFull,MSEmymodFull,SSIMMyModpatch,MSEMuymodpatch=SSIM('/content/dataset/full/GT/full/',pathFirstMod,mod='baseline')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bQpV-jAe-Puv",
        "outputId": "4193121a-f604-422a-c1c0-fa43e2dce8ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model from: /content/PerceptualSimilarity/lpips/weights/v0.1/alex.pth\n",
            "Distance su tutta immagine : 0.0162030299504598\n",
            "Distance su patch : 0.03977130232630549\n",
            "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model from: /content/PerceptualSimilarity/lpips/weights/v0.1/alex.pth\n",
            "Distance su tutta immagine : 0.020233110586802164\n",
            "Distance su patch : 0.04143862788741653\n"
          ]
        }
      ],
      "source": [
        "DISTBaselineFull,DISTBaselinePatch=DIST('/content/dataset/full/GT/full/','/content/results/Full/EGVSR_iter12000/full/',mod='baseline')\n",
        "DISTMymodFull,DISTMymodPatch=DIST('/content/dataset/full/GT/full/',pathFirstMod,mod='baseline')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H8drxfOTvmAC"
      },
      "source": [
        "###Results Second Method"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if best:\n",
        "  pathSecondMod='/content/results/MySecondMod/EGVSR_iter12000'\n",
        "else:\n",
        "  pathSecondMod='/content/results/MySecondMod/MyG_iter12000'"
      ],
      "metadata": {
        "id": "bpBPKUTGJU5U"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UlrqYR0Ovo6M",
        "outputId": "dac283c9-e32a-4e4e-a681-ec4ff014169c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PSNR calcolato su patch Ã¨: 25.192784819883457\n",
            "PSNR calcolato intera immagine Ã¨: 29.60457198583706\n",
            "PSNR calcolato su patch Ã¨: 24.358896341687725\n",
            "PSNR calcolato intera immagine Ã¨: 28.26041169914008\n"
          ]
        }
      ],
      "source": [
        "PSNRBaselinepatch,baselineFull=PSNR('/content/dataset/full/GT/full/','/content/results/Full/EGVSR_iter12000/full/',mod='baseline')\n",
        "PSNRMyMod,MyModFull=PSNR('/content/dataset/full/GT/full',pathSecondMod)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cQPIXxpFvpBP",
        "outputId": "d10dd972-5373-4ffb-b751-e721ba74e69c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SSIM calcolato su intere immagini Ã¨: 0.9398859157592359\n",
            "SSIM calcolato su Patch Ã¨: 0.8416278529907878\n",
            "SSIM calcolato su intere immagini Ã¨: 0.9303168914797152\n",
            "SSIM calcolato su Patch Ã¨: 0.8123959749807731\n"
          ]
        }
      ],
      "source": [
        "SSIMBaselineFull,MSEbaselineFull,SSIMBaselinepatch,MSEbaselinepatch=SSIM('/content/dataset/full/GT/full/','/content/results/Full/EGVSR_iter12000/full/',mod='baseline')\n",
        "SSIMMymodFull,MSEmymodFull,SSIMMyModpatch,MSEMuymodpatch=SSIM('/content/dataset/full/GT/full',pathSecondMod)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kTgr8oT7vpIV",
        "outputId": "e3406a6f-b0db-469d-8cbe-4f4d6774c3b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model from: /content/PerceptualSimilarity/lpips/weights/v0.1/alex.pth\n",
            "Distance su tutta immagine : 0.0162030299504598\n",
            "Distance su patch : 0.03977130232630549\n",
            "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model from: /content/PerceptualSimilarity/lpips/weights/v0.1/alex.pth\n",
            "Distance su tutta immagine : 0.0438891609509786\n",
            "Distance su patch : 0.042961545892663905\n"
          ]
        }
      ],
      "source": [
        "DISTBaselineFull,DISTBaselinePatch=DIST('/content/dataset/full/GT/full/','/content/results/Full/EGVSR_iter12000/full/',mod='baseline')\n",
        "DISTMymodFull,DISTMymodPatch=DIST('/content/dataset/full/GT/full',pathSecondMod)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "OJz2YgjK_Xr6",
        "NGHhh4RPqWZC",
        "aNWtcRsoqPOn",
        "3SSewQw5_zlq",
        "c4hvf_O89WIn",
        "O5Ol6MJF9bY3",
        "QkoAzjES91ej",
        "xy5Vml1I9_f_",
        "X3Qkq-azfrjY",
        "HZxVM8DjhRgE"
      ],
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "2ddec4b305f843b9aaa7c7b128f4c040": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_937d009a7e334e23be468aa50c726384",
              "IPY_MODEL_a7242a94ab22411dac3a2dc34cb7d844",
              "IPY_MODEL_0d2e2b190af44df3a4b274dd5e44268a"
            ],
            "layout": "IPY_MODEL_10098d7fd8324b94aa87ec58326d3ce5"
          }
        },
        "937d009a7e334e23be468aa50c726384": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8a64b53db0864cc293c35d45ccd0dcb3",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_f082072d01e3475eb4cc06092ae2d33c",
            "value": "100%"
          }
        },
        "a7242a94ab22411dac3a2dc34cb7d844": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a3da4a8583e44c44b579a48362a0c5d0",
            "max": 14808437,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1c308767d2204aba8c1c3f625c46496b",
            "value": 14808437
          }
        },
        "0d2e2b190af44df3a4b274dd5e44268a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7277c97947904319b6c7b68c44150735",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_df6e01414f8d4d13a0a4154fff1f3d92",
            "value": " 14.1M/14.1M [00:00&lt;00:00, 46.4MB/s]"
          }
        },
        "10098d7fd8324b94aa87ec58326d3ce5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8a64b53db0864cc293c35d45ccd0dcb3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f082072d01e3475eb4cc06092ae2d33c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a3da4a8583e44c44b579a48362a0c5d0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1c308767d2204aba8c1c3f625c46496b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7277c97947904319b6c7b68c44150735": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "df6e01414f8d4d13a0a4154fff1f3d92": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}